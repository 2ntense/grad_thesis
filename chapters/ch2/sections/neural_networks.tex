\section{Deep learning}
% TODO add intro

\subsection{Neural network}
Neural networks or artificial neural networks (ANNs), the foundation of deep learning, are inspired by the structre and function of the human brain.
They consist of interconnected nodes, called neurons, organized into layers.
Each neuron receives input from other neurons, performs a weighted sum of these inputs, and apllies an activation function to produce an output.
The connections between neurons have associated weights (also called parameters), which determines the strength of the signal transmitted between them.

\begin{figure}[hbtp]
\centering    
\includegraphics[width=3cm]{example-image-a}
\caption{TODO: neural network image}
\end{figure}

The learning process in ANNs centers around adjusting their weights to minimize the difference between the network's predictions and the actual target values.
This is achieved through algorithms like backpropagation, which calculates the error in the network's prediction and propagates this error back through the layers, adjusting the weights accordingly.
This iterative process allows the network to continuously refine its understanding of the underlying patterns in the data.

\subsection{Convolutional neural network}
Convolutional neural networks (CNNs) are a specialized type of neural network designed to process data with a grid-like structure such as images.

Convolutional neural networks (CNNs) are a specialized class of artificial neural networks that excel processing data with a grid-like structure, such as images.
Unlike traditional neural networks that treat input as a simple sequence of values, CNNs are designed to e

\begin{figure}[hbtp]
\centering    
\includegraphics[width=3cm]{example-image-a}
\caption{TODO: CNN image}
\end{figure}
% TODO complete this section


A Convolutional Neural Network (CNN) is a popular neural network model that has been shown to do well in image-related tasks.
% It is the most widely used model for video compression [16].
CNNs are built by stacking layers of convolving filters with activation functions.
% Sometimes pooling layers performing subsampling are included, and many complete networks also include a dense (fully connected) layer at the end (Figure 2.7).
% Compared to a fully connected layer, a convolutional layer allows the model to handle the large dimensions of an image by dramatically reducing the number of pixels that a certain output is depending on, as well as reducing the number of parameters by forcing the filter coecients to be the same regardless of position.
% Furthermore the convolutional aspect allows the model to focus on features localized in the image.

% \textbf{Multiply-accumulate operation} \\
% The multiply-accumulate operation is an operation that multiplies two numbers and adds them to an accumulator.
% \begin{equation}
%     a \leftarrow a + ( b \times c )
% \end{equation}

% This particular operation is used especially frequently in CNNs due to the stacked convolutional layers.

\subsection{Residual neural network}
% TODO complete this section
Residual neural network (ResNet) is a deep CNN architecture that was proposed in 2015 by Microsoft Research in order to combat the difficulty of training deep CNNs (networks with multiple layers between input and output layers).
% In their proposal they demonstrated that their ResNet outperformed current state of the art networks on the ImageNet test set while being up to 8x deeper.
% Before ResNets, variations of deep CNNs performed best at state of the art benchmarks [17].
% The depth of the CNNs is been proven to be beneficial with deeper CNNs such as VGG-19 [18] outperforming shallower models, but only up until a certain point where the depth yields worse results.
% Naively, one might think that this should not happen as adding $m$ number of layers to a network with $n$ layers could
% in worst case result in same performance as the $m$ added layers learning the identity mapping of previous layers and in best case learn features that the network with $n$ layers was unable to learn.
% However, this is evidently not the case as exemplified in \cref{TODO}.
% This is known as the degradation problem, where the network's ability to propagate information form the foremost layers becomes degraded and therefore loses information as the information travels through the network \cref{TODO} [17].
% The worse performance with more layers cannot be attributed to \textit{overfitting} as the training error (as well as the validation error) is higher than that of the more shallow network.

% \textbf{Skip connection} \\
% The ResNet architecture addresses the aforementioned problem with learning identity mapping by introducing skip connections (also known as shortcut or identity connections), denoted by \textbf{x identity} in \cref{TODO}.

\begin{figure}[hbtp]
\centering    
\includegraphics[width=3cm]{example-image-a}
\caption{TODO: ResNet image}
\end{figure}

\subsection{Model optimizations}
% TODO complete this section
% \begin{itemize}
%     \item Quantization
%     \item Pruning
%     \item ...
% \end{itemize}