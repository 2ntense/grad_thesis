\section{Deep learning}
Deep learning is a specialized branch of machine learning that focuses on training artificial neural networks with many layers.
These layers are the core building blocks that enable deep learning models to learn complex patterns and representations from vast amounts of data.
This approach has led to significant breakthroughs in various AI tasks, outperforming traditional machine learning methods in many applications \cite{lecunDeepLearning2015}.

\begin{figure}[hbtp]
    \centering    
    \input{assets/ann.tex}
    \caption{An artificial neural network with two hidden layers, showing neurons fully connected between consecutive layers.}
    \label{fig:ann}
\end{figure}

In the context of artificial neural networks, a layer is a collection of interconnected nodes, or neurons, that process information (see \cref{fig:ann}).
Each connection between two neurons has an associated weight (also known as a parameter), which is a numerical value that determines the strength of the connection.
These weights, play a crucial role in the learning process of a neural network.
A deep learning model typically consist of multiple layers stacked on top of each other, forming a hierarchical structure.
This structure allows the network to learn increasingly complex representations of the data as it flows through the layers.

\begin{figure}[hbtp]
    \centering    
    \input{assets/neuron.tex}
    \caption{
        A single neuron in an artificial neural network, showing the weighted sum of inputs ($\sum_{i=1}^{3}{w_i \times x_i}$), bias ($b$), activation function ($f$), and output ($y$).
    }
    \label{fig:neuron}
\end{figure}

Each layer performs a specific computation on the data it receives from the previous layer.
A neuron that receives input values from its connected neurons multiplies each value by its corresponding weight (see \cref{fig:neuron}).
These weighted values are then aggregated by the receiving neuron.
This aggregation is a simple summation, resulting in a weighted sum of the input values.
In addition to the weighted sum of inputs, a bias term is also added to the neuron.
The bias acts as an offset, allowing the neuron to activate even when the weighted sum of inputs is zero.
It provides flexibility for the neuron to adjust its output based on the specific task it is learning.
Finally, the neuron applies a non-linear transformation, called an activation function, to the sum of the weighted inputs and the bias to produce its output.

Different types of layers serve different purposes in a deep learning model:
\begin{description}
    \item[Input layer:] 
    The first layer in the network, responsible for receiving the initial input data.
    Each neuron in this layer represents a feature of the input data, such as the pixel values in an image or the words in a sentence.
    \item[Hidden layer:] 
    Intermediate layers that lie between the input and output layers.
    They perform computations on the data received from the input, extracting features and learning increasingly abstract representations.
    Each hidden layer typically consists of many neurons, each applying an activation function to the weighted sum of its inputs.
    The activation function determines whether a neuron ``activates''.
    A neuron is said to be activated when its output is significant enough to contribute to the next layer's computations.
    \Cref{fig:activation_functions} shows activation functions that are commonly used: $\relu$, $\tanh$ and $\sigma$ (sigmoid).

    \begin{figure}[hbtp]
        \centering    
        \input{assets/activation_functions.tex}
        \caption{Commonly used activation functions}
        \label{fig:activation_functions}
    \end{figure} 

    The number of hidden layers and neurons in each layer determines the depth and complexity of the network.
    \item[Output layer:] 
    The final layer, producing the result of the network's computation.
    The number of neurons in the output layer depends on the specific task, such as a classification label for image recognition or predicted value for natural language generation \cite{goodfellowDeepLearning2016}.
\end{description}

ANNs learn by adjusting the weights of the connections between neurons.
This process is called training and involves feeding the network with labeled data and iteratively updating the weights to minimize the difference between the network's predictions and the true labels.
The most common algorithm for training ANNs is backpropagation \cite{rumelhartLearningRepresentationsBackpropagating1986}, which calculates the gradient of the error with respect to each weight and updates the weights accordingly.

\subsection{Convolutional neural network}
Convolutional neural networks (CNNs) are a specialized class of artificial neuron networks designed to process data with inherent spatial structure, such as images.

A key element that sets them apart is the convolutional layer.
CNNs leverage the concept of convolution to extract local features.
The network then uses these extracted features to construct increasingly complex understandings of the input as the data progresses through its layers

\begin{figure}[hbtp]
    \centering    
    \input{assets/2dconv.tex}
    \caption{
        This convolution operation involves a 3x3 kernel sliding across the input matrix.
        At each step, the kernel covers a receptive field (highlighted) and computes a weighted sum of the input values.
        This example shows one such step, resulting in the value 4 in the feature map.
    }
    \label{fig:2dconv}
\end{figure}

A convolution operation involves sliding a learnable filter, or kernel, across the input data (see \cref{fig:2dconv}).
At each position, the filter performs an element-wise multiplication with the corresponding input data and sums the results.
This produces a single value in the output feature map.
By applying different filters, the network learns to detect various patterns like edges and shapes.

\begin{figure}[hbtp]
    \centering    
    \input{assets/maxpooling.tex}
    \caption{
        Max pooling operation with a 2x2 pooling window.
        In this example, the maximum value (3) within the highlighted region of the input feature map is extracted and placed in the pooled feature map.
    }
    \label{fig:maxpooling}
\end{figure}

The output of convolutional layers is then typically passed through pooling layers.
Pooling downsamples the feature map, reducing their dimensionality and making the network more robust to variations in the input.
Common pooling operations include taking the maximum or average value within a local region (see \cref{fig:maxpooling}). 

The use of activation functions introduces non-linearity, which enables the learning of complex patterns.
These activation functions are applied to the output of the convolutional and pooling layers.

Finally, fully connected layers are often used in the final stages of a CNN.
These layers connect every neuron in the previous layer to every layer in the current layer.
This enables high-level reasoning and decision-making based on the extracted features.

The combining of simpler features into more complex ones across layers makes CNNs excel in computer vision tasks like image classification, object detection and image segmentation \cite{krizhevskyImageNetClassificationDeep2017}.

\subsection{Model optimizations}
Reducing the memory footprint of neural networks is an actively researched topic \cite{neillOverviewNeuralNetwork2020, leEfficientNeuralNetworks2023}.
Model compression involves refining a model to ensure it can function on devices with limited computational power, memory, and energy.
The goal is to maintain the model's performance while reducing resource demands.

Quantization focuses on reducing the numerical precision of a neural network's parameters.
Instead of relying on resource-intensive, high-precision floating-point numbers, quantization techniques employ lower-bit representations.
This reduction in precision translates to significant savings in memory and computational costs \autocite{guoSurveyMethodsTheories2018}.

Pruning techniques strategically remove less important components from a neural network.
This can involve eliminating individual weights, entire neurons, or even complete layers.
By simplifying the network architecture, pruning reduces its memory footprint and improves computational efficiency \cite{blalockWhatStateNeural2020}.
Different pruning strategies exist, including unstructured pruning, which removes individual weights based on their importance, and structured pruning, which eliminates entire filters within convolutional layers.

Beyond applying compression techniques to existing models, researchers are actively exploring novel neural network architectures that are inherently more efficient.
One such example is the residual neural network (ResNet) \autocite{heDeepResidualLearning2015}.
ResNets introduce skip connections, which allow the input of a layer to bypass one or more subsequent layers.
This seemingly simple modification has a major impact, enabling the training of much deeper networks while mitigating the vanishing gradient problem \cite{pascanuDifficultyTrainingRecurrent2012}.
Skip connections provide an alternate path for gradients to flow during backpropagation, preventing them from diminishing excessively as they travel through many layers.
This allows ResNets to learn more complex features and achieve higher accuracy with fewer parameters. 

% \begin{figure}[hbtp]
% \centering    
% \includegraphics[width=3cm]{example-image-a}
% \caption{TODO: ResNet image}
% \end{figure}