\subsection{Model compression}
Reducing the memory footprint of neural networks is an actively researched topic \cite{neillOverviewNeuralNetwork2020, leEfficientNeuralNetworks2023}.
Model compression involves refining a model to ensure it can function on devices with limited computational power, memory, and energy.
The goal is to maintain the model's performance while reducing resource demands.

Quantization is a common technique used to reduce the precision of the model's parameters.
This decreases the model's size and speed up the computation since lower precision calculations are less resource-intensive.

Pruning is used to simplify and shrink the size of a model by removing parts of the neural network that contribute little to the output.
It aims to improve computational efficiency and reducing memory footprint. 

Knowledge distillation is a training technique where a smaller model is taught to replicate the performance of a larger model.
The larger model's output serves as a guide for the smaller model, which learns to approximate the same function with fewer parameters.

The architecture of a model can also be optimized using neural network designs that are inherently more efficient.
These architectures use less computationally expensive operations and have fewer parameters, which helps to maintain performance while reducing the model's size.

While these model compression techniques can be very beneficial for deploying large models on resource-constrained environments, they do come with disadvantages.
Notably, model compression may lead to the degradation of the model's accuracy.
Compressed models may not retain the full predictive power of the original, larger model, especially if the compression was done aggressively.
In certain critical application like medical diagnosis and autonomous driving, a reduction in accuracy could lead to incorrect decisions, with potentially severe consequences.
