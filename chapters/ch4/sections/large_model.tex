\section{Large model}
The memory capacity of the \graicore{} is limited to \SI{36}{MiB}.
Large models have high memory requirements, exceeding the memory capacity of the \graicore{}, even after applying model optimizations.
This limitation has a major implication and that is the inability to deploy such large models.

We employ splitting of a large model into groups of layers to allow for a large model to be partitioned in parts.
Each part is expected to fit on the \graicore{} and whenever the large model is invoked for execution, each part configured and processed by the \graicore{} sequentially.

We acknowledge that a optimal partitioning strategy for an arbitrary large model is a complex problem to solve.
In \cref{ch:8}, we present a comprehensive formal description of the problem.

% The partitioning strategy applied is 
% TODO
%%%

In this section, we analyze the execution of such large models.
As a large model can be of any size that can be split with in an arbitrary amount of parts, and with each part having its own processing latency, no bandwidth requirement is suitable for all large models.

\subsection{Bandwidth analysis}
To target a specific input frame rate with a large model, we require information for the following components:
\begin{itemize}
    \item Number of model parts
    \item Configuration time for each model part. Which depends on:
    \begin{itemize}
        \item Write bandwidth
        \item Amount of data to write for each model part
    \end{itemize}
    \item Processing time for each model part
\end{itemize}

We approximate the maximum reachable input frame rate ($f$) as follows:
\begin{align*} 
    \lattotal &= \sum_{p \in P}^{}{\latconf(p) + \latproc(p)} \\
    f &= \lattotal^{-1}
\end{align*}

\begin{eqexpl}[15mm]
    \item{$\lattotal$} total latency
    \item{$P$} set of model parts
    \item{$\latconf(p)$} configuration time for model part $p$
    \item{$\latproc(p)$} processing time for model part $p$
\end{eqexpl}

For example, assume we have a large model partitioned into four parts with each part requiring \SI{20}{MiB} to be written and has \SI{5}{ms} of processing latency.
The total latency is then only dependent on the available bandwidth, as it influences the configuration time:
\begin{equation*}
    % L = \left( \frac{\SI{20}{MiB}}{\textrm{BW}} + \SI{5}{ms} \right) \times 4
    f = \left[ \left( \frac{\SI{20}{MiB}}{\textrm{BW}} + \SI{5}{ms} \right) \times 4 \right]^{-1}
\end{equation*}

\begin{eqexpl}[15mm]
    \item{$\textrm{BW}$} available bandwidth
\end{eqexpl}

\begin{figure}
    \centering
    \input{assets/large_model_bandwidth_analysis_example}
    \caption{Reachable frame rates with varying bandwidths for an example large model}
    \label{fig:large_model_bandwidth_analysis_example}
\end{figure}

\Cref{fig:large_model_bandwidth_analysis_example} shows that there is an horizontal asymptote at \SI{50}{FPS}\footnote{ $\lim_{x \to \infty} \left[ \left( \frac{\SI{20}{MiB}}{x} + \SI{5}{ms} \right) \times 4 \right]^{-1} = \SI{50}{Hz}$}.
Due to this, it is not possible to target an input frame rate of \SI{60}{FPS} for this model.
Even with an infinite write bandwidth, we can never reach \SI{60}{FPS}.
This limitation is due to the fixed processing latency of the four model parts (\SI{20}{ms}).
An input frame rate of \SI{30}{FPS} is possible and requires a write bandwidth of at least \SI{6}{GiB/s}.

With the bandwidth of the original \confignoc{} (\SI{763}{MiB/s}), we can target up to \SI{8}{FPS}\footnote{$\left[ \left( \frac{\SI{20}{MiB}}{\SI{763}{MiB/s}} + \SI{5}{ms} \right) \times 4 \right]^{-1} \approx \SI{8}{Hz}$}.

\subsection{Execution of the ResNet-101 model}
We will be looking at the execution of a large model named ResNet-101 on the \graicore{}.
The ResNet-101 model is a deep neural network commonly used for computer vision applications \autocite{heDeepResidualLearning2015}.
We will be using the ResNet-101 model from the \textit{Keras} library \cite{KerasDocumentationResNet} pretrained on the ImageNet database \cite{russakovskyImageNetLargeScale2014}.
The ResNet-101 model has around 44.5 million parameters in total.
% An important feature of the ResNet architecture is the use of skip connections \cref{TODO}.

Due to the size (amount of parameters) of the ResNet-101 model, it is not possible to load the model on the \graicore{} at once.
This is still the case when representing the parameters with low-precision data types like 8-bit integers or floats (this also known as quantization).
Other solutions, such as model compression techniques exist to decrease the size of the model.
However, as explained earlier, this has its disadvantages such as decreased inference accuracy.

% TODO: refer to the DEFER paper
\begin{figure}[hbtp]
    \centering    
    \includegraphics[width=3cm]{example-image-a}
    \caption{TODO: partitioning of a model diagram}
\end{figure}

To allow the model to fit on the \graicore{} we will be looking at partitioning the model into smaller parts.
A model part is capable of being (completely) loaded on the \graicore{}.
By sequentially loading and processing each of the model parts, the full functionality of the original (large) model can be emulated.
The output data of each model part must be buffered outside the local memories, and will be used as input for the subsequent model part.
The amount of data to be buffered is dependent on the output shape of the final layer of a part.

% Due to ResNet's extensive use of skip connections, it is not straightforward to split/partition the model at any location. We therefore only consider locations in the network where it is easy to split. In most cases this is at locations right before or after a split connection.

\begin{figure}[hbtp]
    \centering
    \includegraphics[angle=90, width=\linewidth]{assets/resnet101_residual_blocks.pdf}
    \caption{
    Snippet of a visualization of the ResNet-101 network.
    The red rectangles represent the residual blocks.
    }
    \label{fig:resnet101_residual_blocks}
\end{figure}

The ResNet architecture introduces so-called residual blocks (see \cref{fig:resnet101_residual_blocks}).
A residual block contains a collection of layers with a special type of connection from the beginning of the block until the end of the block.
This connection is also called a skip connection.
Due to these skip connections, it is not straightforward to split the ResNet-101 model at any location.
Splitting the model between two layers where also a skip connection resides, will introduce a few difficulties.
Firstly, you will require to buffer multiple outputs.
That is, the output data of the layer just before the split point and the output data of the skip connection.
Secondly, the two outputs need to be propagated to their respective destination layers.
These two outputs can have the same or different destination layer.
If they have different destinations, the output of the skip connection needs to be buffered until the network reaches a later stage.

In the context of the \graicore{}, this adds challenges to the system.
Not only does this increase the instantaneous total memory usage, it also increases the energy usage.
More energy is consumed due to the increased data to be buffered.
This is especially evident due to the need to buffer the data on some external memory.
Next to this buffering problem, software support must also be provided.
The current system only allows for a single input to the model. 

Due to these complexities, we keep the partitioning of the ResNet-101 model simple and only consider partitioning the network at locations where no skip connections appear.
Naturally, the main drawback of this is the reduces number of locations where partitioning can occur.

\begin{figure}[hbtp]
    \centering
    \subcaptionbox{MACs per residual block \label{fig:resnet101_macs}}[\textwidth]{
        \input{assets/resnet101_macs}
    }
    \subcaptionbox{Number of parameters per residual block \label{fig:resnet101_params}}[\textwidth]{
        \input{assets/resnet101_params}
    }
    \caption{
        MACs and the number of parameters per residual block.
        The tuple $(x, y)$ represents the residual block at index $y$ of the stage at index $x$.
        \textit{Head} and \textit{tail} contain the remaining layers at the start and end of the network respectively.
    }
    \label{fig:resnet101_stats}
\end{figure}

The ResNet-101 model contains five so-called stages.
A stage is a collection of consecutive layers.
Only the first stage does not include any residual blocks.
The other stages contain different amounts of residual blocks as shown in \cref{fig:resnet101_stats}.

If we opt to split the network based on computational complexity, we look at the amount of multiply-accumulate (MAC) operations of the layers.
A MAC operation is essentially a multiplication operation followed by a addition operation.
Since we only perform splits between residual blocks, we combine the MACs of each layer in the residual blocks.
In \cref{fig:resnet101_macs}, we observe that the amount of MACs for each residual block is relatively similar.
We see small peaks at the start of each stage.
This is due to an additional convolution layer in the first residual block of a stage.
% A model or model part with more MACs generally is more heavy to run than those with less MACs (lower .
Generally, a model or model part with high number of MACs take longer to process fully.

In \cref{fig:resnet101_params}, we observe that the distribution of the parameters do vary significantly over the residual blocks.
The graph shows that the last few residual blocks carry a significant portion of the total parameters.
Again, we see small peaks at the start of each stage.
Naturally, a model part with a higher number of parameters will require more memory space.
Due to the uneven distribution of the parameters we have less flexibility with partitioning. 

\subsubsection{Splitting}
The ResNet-101 model has a total parameter count of around \SI{44.5}{M}.
Even if the parameters are quantized to an 8-bit datatype, the model will still not fit on the \graicore{} as a whole.
Weights stored on the \graicore{} require one extra bit.
% This extra bit is used as a mask that activates or deactivates the associated weight.
This extra bit is used to indicate whether a weight is removed (i.e., pruned) or not.
However, since we are using the original ResNet-101, no pruning has been applied to the weights.
We can roughly say that the network requires \SI{47.7}{MiB}\footnote{$\frac{\num{44496488} \times 9}{8} \times \frac{1}{2^{20}}$} of memory capacity for the parameters when quantized to 8 bits.
However, this does not include any mandatory header information and buffer space when mapping the network for the \graicore{}. 

We use the in-house compiler to find a valid mapping for the \graicore{}, with focus on low processing latency.
The compiler also outputs files necessary for simulating the networks with \graipefruit{}. \graipefruit{} is used to approximate the processing latency of each part.
Note that the compiler is a complex piece of software.
Since it accepts many parameters that affects the processing latency, it is difficult to find an optimal mapping. 
% The in-house compiler for the \graicore{} is complex and we will not get into detail.
In a nutshell, the compiler first optimizes the model by converting it to a so-called \textit{GrAI Model}.
Among other things, the optimized model slightly changes the architecture of the network and optionally performs post training quantization \autocite{krishnamoorthiQuantizingDeepConvolutional2018} for the weights.
The amount of MACs and parameters stay roughly the same.
Afterwards, it will map the layers of the model to the neuron cores.
The compiler can be instructed to optimize for inference latency.
One of the optimization is, for example, mapping a single layer to multiple neuron cores to exploit parallelism.

One example partitioning is done as follows.
The splitting points are performed right after the following residual blocks: $(4, 6)$, $(4, 12)$, $(4,21)$ and $(5,1)$.
The ResNet-101 model is therefore partitioned in five parts.
The compiler is able to compile each of the parts, indicating that they are configurable on the \graicore{}.
The processing latency for each of the parts are then estimated using the \graipefruit{} simulator.
An attempt has been made to instruct the compiler to optimize the parts for processing latency.

\begin{table}[hbtp]
\centering
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{Part}  & \textbf{To write (MiB)} & \textbf{Latency (ms)} \\ \midrule
1              & 28.75                   & 7.08                  \\
2              & 20.06                   & 2.65                  \\
3              & 30.44                   & 4.01                  \\
4              & 17.71                   & 2.34                  \\
5              & 15.54                   & 2.04                  \\ \midrule
\textbf{Total} & 112.51                  & 18.12                 \\ \bottomrule
\end{tabular}
\caption{Statistics of the example partitioning of the ResNet-101 model in 5 parts}
\label{tab:resnet101_5parts}
\end{table}

Furthermore, the compiler also provides us information about how much data must be written to each of the neuron cores.
Note that the compiler can map a layer to multiple neuron cores.
This requires the weights to be written to all neuron cores that require them.
This means that the \graicore{} contains duplicate weights.
The total data to write and the processing latency for each part is shown in \cref{tab:resnet101_5parts}.

\begin{figure}[hbtp]
    \centering
    \input{assets/large_model_bandwidth_analysis_example_resnet101}
    \caption{Reachable frame rates with varying bandwidths for the example partitioned ResNet-101 model}
    \label{fig:large_model_bandwidth_analysis_example_resnet101}
\end{figure}

The sum of the processing latencies is \SI{18.12}{ms}.
The available time for the configuration of all parts is dependent on the target frame rate.
We observe that it is not possible to target a frame rate of \SI{60}{FPS}.
This is due to the total processing latency exceeding the frame time of \SI{16.67}{ms}. 
For a target frame rate of \SI{30}{FPS}, we have a frame time of \SI{33.33}{ms}.
For the configuration of the parts, we have a total time budget of \SI{15.12}{ms}\footnote{$\SI{33.33}{ms} - \SI{18.12}{ms}$}.
To write all the \SI{112.51}{MiB} to configure the each model part, we require a bandwidth of at least \SI{7441}{MiB/s}\footnote{$\frac{\SI{112.51}{MiB}}{\SI{15.12}{ms}}$}.
\Cref{fig:large_model_bandwidth_analysis_example_resnet101} shows the reachable frame rates with different bandwidths.

% The compiler accepts many parameters that can affect the performance (e.g., inference latency or power usage) when the model is running on the \graicore{}.




%%%

% There are many ways to partition a model. 
% Amount of parts (there is a minimum)
% Distribution of parameters
% Distribution of FLOPS/MACs

% We use the in-house compiler to find a valid mapping for the \graicore{}, with focus on low processing latency. The compiler also outputs files necessary for simulating the networks with \graipefruit{}. \graipefruit{} is used to approximate the processing latency of a network.
% Note that the compiler is a complex piece of software. Since it accepts many parameters, it is difficult to find an optimal mapping. 
