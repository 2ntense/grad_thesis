\section{Future work}
While this study demonstrated the effectiveness of our approach using a sequential configuration and processing scheme, a promising avenue for future research lies in exploring pipelined parallelism.
By enabling concurrent configuration and processing, the system can dynamically reconfigure on-chip memory locations that are no longer required by the current input, even while the current input is still being processed.
This pipelined approach has the potential to significantly increase inference bandwidth, allowing the system to process a higher volume of frames per unit of time.
However, it also introduces complexities in terms of scheduling, resource management, and synchronization that need to be carefully addressed.

Furthermore we focussed was on minimizing inference latency through model partitioning.
Another important consideration for resource-constrained devices is energy efficiency.
Future work will investigate energy-aware partitioning strategies, which may involve different partitioning points and compiler parameters compared to latency-driven approaches.
This is motivated by the understanding that minimizing latency does not necessarily correlate with minimizing energy consumption.

Finally, future research will thoroughly examine the trade-offs between performance, density, reliability, and cost associated with LPDDR5X memory and PCM.
This analysis will involve a comprehensive evaluation of how the densities of these memory technologies affect cost-effectiveness, especially in terms of required silicon area.
Furthermore, to provide a more complete understanding of the energy consumption, future work will also investigate the contribution of static energy, especially considering the varying static power properties of DRAM and NVM technologies.