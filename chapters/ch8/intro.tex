The increasing complexity of AI models demands efficient deployment strategies, especially on resource-constrained edge devices.
These device's limited memory and processing power necessitates the need for partitioning large AI models into smaller, manageable units for execution.

This chapter focuses on the problem of partitioning large models for energy-efficient inference on the \graicore{}.
We formulate the problem of model partitioning as a multi-objective optimization, aiming to minimize energy consumption while adhering to memory constraints and latency requirements.