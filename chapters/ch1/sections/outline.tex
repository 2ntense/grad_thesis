\section{Outline}
% This thesis is organized in the following way.
% \Cref{ch:2} introduces relevant background information about topics that are relevant for the reader for later chapters.
% \Cref{ch:3} provides an analysis of the current \confignoc{}, a crucial component of the system that allows for configuration.
% It provides an overview of its architecture and discusses its bandwidth.
% This chapter provides us with an understanding of its shortcomings and its rooms for improvements.
% \Cref{ch:4} provides an analysis of the execution of multiple AI models and large models on the \graicore{}.
% It discusses requirements that are required for running these models.
% This chapter provides us with the requirements that are needed for the \confignoc{} to be adapted to.
% \Cref{ch:5} presents a list of improvements to the \confignoc{} that improves its bandwidth for the configuration process.
% The improvements are accompanied with an analysis that examines their effects on bandwidth.
% It concludes with a proposed solution for an improved \confignoc{} that is expected to be sufficient for the execution of multiple AI models and large models on the \graicore{}.
% \Cref{ch:6} presents a power analysis of model execution with various models with focus on the \confignoc{}.
% It gives us an understanding of the added costs of run-time configuration.
% \Cref{ch:7} provides a discussion about various methods of improving the efficiency of the configuration process.
% Their potential improvements to the configuration process is presented.
% \Cref{ch:8} presents a formal problem description of partitioning large models.
% This provides us with an understanding of solving the problem of partitioning large models for energy-efficient inference.
% \Cref{ch:9,ch:10} present the concluding remarks of the thesis along with future works that are of interest for further research.

This thesis begins by introducing key background concepts in \cref{ch:2}, laying the foundation for subsequent analyses.
\Cref{ch:3} investigates the current \confignoc{}, a crucial component for data communication that allows for system configuration.
We will explore its architecture, analyze its bandwidth, and identify areas for bandwidth improvement.

In \cref{ch:4}, we focus on the execution of multiple and large AI models on the \graicore{}.
This analysis reveals the requirements for running these models, which in turn inform the necessary adaptations for the \confignoc{}.

\Cref{ch:5} proposes several \confignoc{} enhancements aimed at improving bandwidth during configuration.
Each improvement is accompanied by a thorough analysis of its impact.
The chapter concludes with a proposed solution for an enhanced \confignoc{} designed to support the execution of multiple and large AI models.

\Cref{ch:6} provides an energy analysis of model configuration, highlighting the costs associated with configuration. 
We then explore various methods for improving the efficiency of the configuration process in \cref{ch:7}, evaluating their potential benefits.

% \Cref{ch:8} presents a formal problem description for partitioning large models.
% This provides us with an understanding of solving the problem of partitioning large models for energy-efficient inference.

Finally, \cref{ch:9} offer concluding remarks and outline future works that are of potential interest for future research.




