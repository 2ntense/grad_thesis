\section{Research question}

Due to the limited local memory of the \graicore{}, it can only support AI models of a certain size.
This limitation becomes a roadblock for scenarios where larger models are needed for improved accuracy or when multiple models need to be deployed simultaneously.
The introduction of external memory is a potential solution to this challenge, but it requires careful integration to maintain the energy efficient, high-speed and low-latency performance expected from edge devices.

Ultimately, this project aims to answer the following research question:

\textbf{How can the architectural design of the \graicore{} be adapted to enable seamless reconfiguration?}

To address this overarching question, the project will explore several sub-research questions:

\begin{itemize}
    \item \textbf{Supporting multiple models:}
    \begin{itemize}
        \item What modifications are necessary to support switching between multiple AI models for videos up to \SI{60}{FPS}?
    \end{itemize}
    \item \textbf{Supporting large models:}
    \begin{itemize}
        \item What strategies can be employed to support the execution of large AI models?
    \end{itemize}
    \item \textbf{Sustainable power consumption:}
    \begin{itemize}
        \item How can sustainable power consumption be ensured while maintaining performance?
    \end{itemize}
\end{itemize}