\section{Contributions}
% This work offers a comprehensive analysis of the \graicore{}'s current configuration architecuture, highlighting its limitations, particularly when tasked with executing multiple and large models.
% Through extensive analysis, we identify the system requirements necessary for enabling execution of these models.
% To address the limitations, this work proposes significant improvements to \graicore{}'s communication system, focusing on bandwidth enhancements to meet the demanding requirements for the execution of the models.

% A key contribution is the analysis of a model partitioning, a technique for enabling large models to be executed on the \graicore{}.
% This method facilitates the execution of large models by dividing them into smaller, more manageable units.
% By formally defining the complex problem of partitioning large models for low latency inference, we establish a framework for developing effective solutions.

% Furthermore, the research includes a thorough energy analysis of the system, examining both the configuration process and the energy consumption associated with processing realistic and applicable models.
% This analysis provides valuable insights into the system's efficiency and potential areas for optimization.
% Additionally, we explore potential avenues for further optimization, focusing on improving the efficiency of model configuration and reducing overall energy consumption.

The work presented in this thesis aims to address the challenges associated with the efficient execution of large and multiple models on the \graicore{}.
To achieve this goal, we have made several key contributions:
\begin{itemize}
    \item 
    A comprehensive analysis of the \graicore{}'s current configuration architecture, highlighting its limitations when executing multiple and large models.
    \item
    Identification of system requirements necessary for enabling execution of these models through extensive analysis.
    \item
    Significant improvements to the \graicore{}'s configuration architecture, focusing on bandwidth enhancements to meet requirements for model execution.
    \item
    Analysis of a model partitioning technique, which enables the execution of large models by dividing them into smaller, more manageable units.
    \item
    Formal definition of the complex problem of partitioning large models for low latency inference, establishing a framework for developing effective solutions.
    \item
    Thorough energy analysis of the system, examining both configuration process and energy consumption associated with processing realistic models.
    \item
    Exploration of potential avenues for further optimization, focusing on improving model configuration efficiency and reducing overall energy consumption.
\end{itemize}