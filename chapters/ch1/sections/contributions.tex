\section{Contributions}
% The contributions of this work are summarized in the following points.

% \begin{itemize}
%     \item 
%     Analysis of the current \confignoc{} and presenting its limitations.
%     \item
%     % Requirement analysis of executing multiple and large models on the GrAICore.
%     Analysis of executing multiple and large models on the \graicore{}, obtaining requirements for the system to support.
%     \item
%     % Proposal of improvements to the \confignoc{} for bandwidth improvements to reach high bandwidth configuration
%     Proposal of improvements to the \confignoc{} for bandwidth improvements to reach high bandwidth configuration, meeting the requirements.
%     \item
%     Analysis of model partitioning, enabling the splitting of a large model to make them run on the \graicore{}.
%     \item
%     Power analysis of the system for configuration and processing of a model with practical models.
%     \item
%     Analysis of further improvements for configuration efficiency
%     \item
%     A formal problem description describing complex problem of large model partitioning for energy-efficienct inference on the \graicore{}.
% \end{itemize}

% This work offers a comprehensive analysis of the current \confignoc{}, highlighting its limitations, particularly when tasked with executing multiple and large models on the \graicore{}.
This work offers a comprehensive analysis of the \graicore{}'s current architecuture, highlighting its limitations, particularly when tasked with executing multiple and large models.
Through extensive analysis, we identify the system requirements necessary for enabling execution of these models.
To address the limitations, this work proposes significant improvements to the NoC, focusing on bandwidth enhancements to meet the demanding requirements for the execution of the models.

A key contribution is the analysis of a model partitioning, a technique for enabling large models to be executed on the \graicore{}.
This method facilitates the execution of large models by dividing them into smaller, more manageable units.
By formally defining the complex problem of partitioning large models for low latency inference, we establish a framework for developing effective solutions.

Furthermore, the research includes a thorough energy analysis of the system, examining both the configuration process and the energy consumption associated with processing realistic and applicable models.
This analysis provides valuable insights into the system's efficiency and potential areas for optimization.
Additionally, we explore potential avenues for further optimization, focusing on improving the efficiency of model configuration and reducing overall energy consumption.

% Finally, by formally defining the complex problem of partitioning large models for energy-efficient inference, we establish a framework for future research and development in this field.
