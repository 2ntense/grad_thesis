\section{Motivation}
Edge devices are often the frontline for data collection and user interaction, making them crucial for applications that require real-time processing, such as facial recognition on smartphones or decision-making in autonomous vehicles.
These devices are characterized by their compact size and energy efficiency.
However, the compute capacities of edge devices are considerably lower than those of dedicated AI cloud servers.
This presents a significant challenge for deploying sophisticated AI models directly onto edge devices.
Given the constraints of memory and processing power on the edge, it is crucial to adapt models to operate within these limitations while striving to maximize their efficacy.

To bridge this gap, AI researchers and engineers have developed several model compression techniques \autocite{neillOverviewNeuralNetwork2020}.
One such technique is quantization, which reduces the precision of a model's numerical parameters.
For example, instead of using 32-bit floating-point numbers, a model might use 16-bit or even 8-bit representations, which can drastically cut down the memory requirement and computational costs.
Another technique is pruning, which removes parameters that contribute little to the model's outputs.
The challenge with pruning is to identify which parts of the model can be removed with the smallest impact on its performance.
These model optimization techniques are crucial for maintaining a balance between performance and efficiency.
They aim to minimize the impact on the model's accuracy and predictive capabilities while significantly reducing its memory footprint and computational requirements.

In addition, hardware innovations have emerged to reduce the gap between the computational demands of AI models and the capabilities of edge devices.
The emergence of edge AI accelerators is a clear indicator of this trend.
These specialized processors are engineered to perform the matrix operations and linear algebra that are fundamental to AI computations with high efficiency and low power consumption.
By integrating these accelerators into edge devices, it is possible to execute complex AI models more effectively within the stringent constraints of power, space, and computing resources typical of edge environments.

Although optimization techniques can reduce the memory and computational demands of AI models, the on-chip memory in edge AI accelerators is inherently limited due to their compact size and design constraints aimed at conserving energy and space.
This limitation can be a significant bottleneck, especially as AI models become more complex and require more data to function due to the increase in the number of parameters.
This becomes particularly pronounced when dealing with edge AI accelerators that necessitate preloading model data onto their on-chip memory before inference can begin.

To overcome these limitations, there is a growing interest in exploring off-chip memory solutions that can be integrated with edge AI accelerators.
Compared to on-chip memory options like SRAM, which are often more expensive and bulky, off-chip memory such as DRAM offers a cost-effective and compact alternative \cite{hennessyComputerArchitectureSixth2017}.
Due to the increased density of off-chip memory, it can provide additional storage capacity and allows edge devices the potential to support larger AI models and/or multiple models.
This is particularly beneficial for applications that require high accuracy, as supporting larger models can lead to improved performance compared to using smaller, compressed models.
Furthermore, the ability to execute multiple models during runtime, opens up new possibilities for diverse functionalities and use cases.
Therefore, expanding the memory capacity through external solutions is an effective way to extend the potential of AI at the edge.
