\section{Motivation}
Edge devices are often the frontline for data collection and user interaction, making them crucial for applications that require real-time processing, such as facial recognition on smartphones or decision-making in autonomous vehicles.
These devices are characterized by their compact size and energy efficiency.
However, the compute capacities of edge devices are considerably lower than those of dedicated AI cloud servers.
This presents a significant challenge for deploying sophisticated AI models directly onto edge devices.
Given the constraints of memory and processing power on the edge, it is crucial to adapt models to operate within these limitations while striving to maximize their efficacy.

To bridge this gap, AI researchers and engineers have developed several model compression techniques \autocite{neillOverviewNeuralNetwork2020}.
One such technique is quantization, which reduces the precision of a model's numerical parameters.
For example, instead of using 32-bit floating-point numbers, a model might use 16-bit or even 8-bit representations, which can drastically cut down the memory requirement and computational costs.
Another technique is pruning, which removes parameters that contribute little to the model's outputs.
This is like removing the least essential parts of a complex machine without affecting its overall functionality.
The challenge with pruning is to identify which parts of the model can be removed with the smallest impact on its performance.
These model optimization techniques are crucial for maintaining a balance between performance and efficiency.
They aim to minimize the impact on the model's accuracy and predictive capabilities while significantly reducing its memory footprint and computational requirements.

In addition, hardware innovations have emerged to reduce the gap between the computational demands of AI models and the capabilities of edge devices.
The emergence of edge AI accelerators is a clear indicator of this trend.
These specialized processors are engineered to perform the matrix operations and linear algebra that are fundamental to AI computations with high efficiency and low power consumption.
By integrating these accelerators into edge devices, it is possible to execute complex AI models more effectively within the stringent constraints of power, space, and computing resources typical of edge environments.

Although optimization techniques can reduce the memory and computational demands of AI models, the on-chip memory in edge AI accelerators is inherently limited due to their compact size and design constraints aimed at conserving energy and space.
This limitation can be a significant bottleneck, especially as AI models become more complex and require more data to function due to the increase in the amount of parameters.
This becomes particularly pronounced when dealing with edge AI accelerators that necessitates pre-loading model data onto their on-chip memory before inference can begin.

To overcome these limitations, there is a growing interest in exploring off-chip memory solutions that can be integrated with edge AI accelerators.
Off-chip memory provides additional storage capacity and allows edge devices to support larger AI models or multiple models without compromising their accuracy performance.
Furthermore, supporting large models on edge devices typically results in better accuracy compared to using smaller, compressed models.
Additionally, switching between different models during runtime allows for a wider array of functionalities.
Therefore, expanding the memory capacity through external solutions is an effective way to extend the potential of AI at the edge.
