Recent trends in artificial intelligence (AI) development indicate a substantial surge in the complexity and size of AI models since the advent of the Deep Learning Era \cite{EpochDatabaseVisualization}.
Sophisticated state-of-the-art AI models, such as those used in natural language processing and image recognition, can vary widely in complexity, with some containing as few as thousands of parameters and others exceeding a trillion.
These parameters are the fundamental components that AI models manipulate to learn from data and make predictions or decisions.
Processing such huge models requires an extensive amount of computational power.
High-performance computing systems with powerful GPUs and large memory pools are typically employed to train these models as well as to perform inference using them.
However, the deployment of AI solutions is not limited to these high-resource environments.
There is a significant push to bring AI capabilities closer to the end-users, which is where edge computing comes into play.
This shift is primarily motivated by the need for reduced latency, ensuring that decisions and actions can be taken almost instantaneously, and by the importance of low power consumption.


