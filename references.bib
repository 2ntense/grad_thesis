@inproceedings{abtsAchievingPredictablePerformance2009,
  title = {Achieving Predictable Performance through Better Memory Controller Placement in Many-Core {{CMPs}}},
  booktitle = {Proceedings of the 36th Annual International Symposium on {{Computer}} Architecture},
  author = {Abts, Dennis and Enright Jerger, Natalie D. and Kim, John and Gibson, Dan and Lipasti, Mikko H.},
  year = {2009},
  month = jun,
  pages = {451--461},
  publisher = {ACM},
  address = {Austin TX USA},
  doi = {10.1145/1555754.1555810},
  isbn = {978-1-60558-526-0}
}

@techreport{ARM_AXI_Specification,
  title = {{{AMBA}}{\textregistered} {{AXI}}™ and {{ACE}}™ Protocol Specification},
  author = {{ARM}},
  year = {2013},
  month = feb,
  number = {IHI 0022E},
  institution = {ARM}
}

@article{bakhodaDesigningOnchipNetworks2013,
  title = {Designing On-Chip Networks for Throughput Accelerators},
  author = {Bakhoda, Ali and Kim, John and Aamodt, Tor M.},
  year = {2013},
  month = sep,
  journal = {ACM Transactions on Architecture and Code Optimization},
  volume = {10},
  number = {3},
  pages = {1--35},
  issn = {1544-3566, 1544-3973},
  doi = {10.1145/2512429},
  abstract = {As the number of cores and threads in throughput accelerators such as Graphics Processing Units (GPU) increases, so does the importance of on-chip interconnection network design. This article explores throughput-effective Network-on-Chips (NoC) for future compute accelerators that employ Bulk-Synchronous Parallel (BSP) programming models such as CUDA and OpenCL. A hardware optimization is ``throughput effective'' if it improves parallel application-level performance per unit chip area. We evaluate performance of future looking workloads using detailed closed-loop simulations modeling compute nodes, NoC, and the DRAM memory system. We start from a mesh design with bisection bandwidth balanced to off-chip demand. Accelerator workloads tend to demand high off-chip memory bandwidth which results in a many-to-few traffic pattern when coupled with expected technology constraints of slow growth in pins-per-chip. Leveraging these observations we reduce NoC area by proposing a ``checkerboard'' NoC which alternates between conventional               full               routers and               half               routers with limited connectivity. Next, we show that increasing network terminal bandwidth at the nodes connected to DRAM controllers alleviates a significant fraction of the remaining imbalance resulting from the many-to-few traffic pattern. Furthermore, we propose a ``double checkerboard inverted'' NoC organization which takes advantage of channel slicing to reduce area while maintaining the performance improvements of the aforementioned techniques. This organization also has a simpler routing mechanism and improves average application throughput per unit area by 24.3\%.}
}

@article{barchiFlexibleOnLineReconfiguration2021,
  title = {Flexible {{On-Line Reconfiguration}} of {{Multi-Core Neuromorphic Platforms}}},
  author = {Barchi, Francesco and Urgese, Gianvito and Siino, Alessandro and Cataldo, Santa Di and Macii, Enrico and Acquaviva, Andrea},
  year = {2021},
  month = apr,
  journal = {IEEE Transactions on Emerging Topics in Computing},
  volume = {9},
  number = {2},
  pages = {915--927},
  issn = {2168-6750, 2376-4562},
  doi = {10.1109/TETC.2019.2908079}
}

@article{benSurveyNetworkOnChipTools2013,
  title = {A {{Survey}} of {{Network-On-Chip Tools}}},
  author = {Ben, Ahmed and Ben, Slim},
  year = {2013},
  journal = {International Journal of Advanced Computer Science and Applications},
  volume = {4},
  number = {9},
  issn = {2158107X, 21565570},
  doi = {10.14569/IJACSA.2013.040910}
}

@misc{blalockWhatStateNeural2020,
  title = {What Is the {{State}} of {{Neural Network Pruning}}?},
  author = {Blalock, Davis and Ortiz, Jose Javier Gonzalez and Frankle, Jonathan and Guttag, John},
  year = {2020},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2003.03033},
  abstract = {Neural network pruning---the task of reducing the size of a network by removing parameters---has been the subject of a great deal of work in recent years. We provide a meta-analysis of the literature, including an overview of approaches to pruning and consistent findings in the literature. After aggregating results across 81 papers and pruning hundreds of models in controlled conditions, our clearest finding is that the community suffers from a lack of standardized benchmarks and metrics. This deficiency is substantial enough that it is hard to compare pruning techniques to one another or determine how much progress the field has made over the past three decades. To address this situation, we identify issues with current practices, suggest concrete remedies, and introduce ShrinkBench, an open-source framework to facilitate standardized evaluations of pruning methods. We use ShrinkBench to compare various pruning techniques and show that its comprehensive evaluation can prevent common pitfalls when comparing pruning methods.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {FOS: Computer and information sciences,Machine Learning (cs.LG),Machine Learning (stat.ML)}
}

@article{chenIncreasingOffchipBandwidth2014,
  title = {Increasing Off-Chip Bandwidth in Multi-Core Processors with Switchable Pins},
  author = {Chen, Shaoming and Hu, Yue and Zhang, Ying and Peng, Lu and Ardonne, Jesse and Irving, Samuel and Srivastava, Ashok},
  year = {2014},
  month = oct,
  journal = {ACM SIGARCH Computer Architecture News},
  volume = {42},
  number = {3},
  pages = {385--396},
  issn = {0163-5964},
  doi = {10.1145/2678373.2665730},
  abstract = {Off-chip memory bandwidth has been considered as one of the major limiting factors to processor performance, especially for multi-cores and many-cores. Conventional processor design allocates a large portion of off-chip pins to deliver power, leaving a small number of pins for processor signal communication. We observed that the processor requires much less power than that can be supplied during memory intensive stages. This is due to the fact that the frequencies of processor cores waiting for data to be fetched from off-chip memories can be scaled down in order to save power without degrading performance. In this work, motivated by this observation, we propose a dynamic pin switch technique to alleviate the bandwidth limitation issue. The technique is introduced to dynamically exploit the surplus pins for power delivery in the memory intensive phases and uses them to provide extra bandwidth for the program executions, thus significantly boosting the performance}
}

@inproceedings{chenPhysicalVsVirtual2010,
  title = {Physical vs. {{Virtual Express Topologies}} with {{Low-Swing Links}} for {{Future Many-Core NoCs}}},
  booktitle = {2010 {{Fourth ACM}}/{{IEEE International Symposium}} on {{Networks-on-Chip}}},
  author = {Chen, Chia-Hsin Owen and Agarwal, Niket and Krishna, Tushar and Koo, Kyung-Hoae and Peh, Li-Shiuan and Saraswat, Krishna C.},
  year = {2010},
  pages = {173--180},
  publisher = {IEEE},
  address = {Grenoble, France},
  doi = {10.1109/NOCS.2010.26},
  isbn = {978-1-4244-7085-3}
}

@misc{chollet2015keras,
  title = {Keras},
  author = {Chollet, Fran{\c c}ois and others},
  year = {2015}
}

@inproceedings{droliaPrecogPrefetchingImage2017,
  title = {Precog: Prefetching for Image Recognition Applications at the Edge},
  shorttitle = {Precog},
  booktitle = {Proceedings of the {{Second ACM}}/{{IEEE Symposium}} on {{Edge Computing}}},
  author = {Drolia, Utsav and Guo, Katherine and Narasimhan, Priya},
  year = {2017},
  month = oct,
  pages = {1--13},
  publisher = {ACM},
  address = {San Jose California},
  doi = {10.1145/3132211.3134456},
  isbn = {978-1-4503-5087-7}
}

@inproceedings{dulloorSystemSoftwarePersistent2014,
  title = {System Software for Persistent Memory},
  booktitle = {Proceedings of the {{Ninth European Conference}} on {{Computer Systems}}},
  author = {Dulloor, Subramanya R. and Kumar, Sanjay and Keshavamurthy, Anil and Lantz, Philip and Reddy, Dheeraj and Sankaran, Rajesh and Jackson, Jeff},
  year = {2014},
  month = apr,
  pages = {1--15},
  publisher = {ACM},
  address = {Amsterdam The Netherlands},
  doi = {10.1145/2592798.2592814},
  isbn = {978-1-4503-2704-6}
}

@misc{EpochDatabaseVisualization,
  title = {Epoch {{Database Visualization}}},
  journal = {Epoch},
  abstract = {Explore Epoch's database of notable machine learning systems and dive into our interactive visualization to inform your research, work and education. Learn about the training compute, parameters, and training dataset sizes of frontier AI systems.},
  howpublished = {https://epochai.org/data/epochdb/visualization}
}

@misc{EpochNotableModels2024,
  title = {Data on Notable {{AI}} Models},
  author = {{Epoch AI}},
  year = {2024}
}

@article{furberSpiNNakerProject2014,
  title = {The {{SpiNNaker Project}}},
  author = {Furber, Steve B. and Galluppi, Francesco and Temple, Steve and Plana, Luis A.},
  year = {2014},
  month = may,
  journal = {Proceedings of the IEEE},
  volume = {102},
  number = {5},
  pages = {652--665},
  issn = {0018-9219, 1558-2256},
  doi = {10.1109/JPROC.2014.2304638}
}

@book{furberSpiNNakerSpikingNeural2020,
  title = {{{SpiNNaker}}: {{A Spiking Neural Network Architecture}}},
  shorttitle = {{{SpiNNaker}}},
  editor = {Furber, Steve and Bogdan, Petrut},
  year = {2020},
  publisher = {Now Publishers},
  doi = {10.1561/9781680836523},
  isbn = {978-1-68083-652-3 978-1-68083-653-0}
}

@article{furhadExtendedDiagonalMesh2015,
  title = {An {{Extended Diagonal Mesh Topology}} for {{Network-on-Chip Architectures}}},
  author = {Furhad, Md. Hasan and Kim, Jong-Myon},
  year = {2015},
  month = oct,
  journal = {International Journal of Multimedia and Ubiquitous Engineering},
  volume = {10},
  number = {10},
  pages = {197--210},
  issn = {19750080},
  doi = {10.14257/ijmue.2015.10.10.21}
}

@inproceedings{glassTurnModelAdaptive1992,
  title = {The {{Turn Model}} for {{Adaptive Routing}}},
  booktitle = {[1992] {{Proceedings}} the 19th {{Annual International Symposium}} on {{Computer Architecture}}},
  author = {Glass, C.J. and Ni, L.M.},
  year = {1992},
  pages = {278--287},
  publisher = {IEEE},
  address = {Gold Coast, Australia},
  doi = {10.1109/ISCA.1992.753324},
  isbn = {978-0-89791-509-0}
}

@book{goodfellowDeepLearning2016,
  title = {Deep Learning},
  author = {Goodfellow, Ian and Courville, Aaron and Bengio, Yoshua},
  year = {2016},
  series = {Adaptive Computation and Machine Learning},
  publisher = {The MIT Press},
  address = {Cambridge, Massachusetts},
  isbn = {978-0-262-03561-3 978-0-262-33737-3}
}

@misc{guoSurveyMethodsTheories2018,
  title = {A {{Survey}} on {{Methods}} and {{Theories}} of {{Quantized Neural Networks}}},
  author = {Guo, Yunhui},
  year = {2018},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.1808.04752},
  abstract = {Deep neural networks are the state-of-the-art methods for many real-world tasks, such as computer vision, natural language processing and speech recognition. For all its popularity, deep neural networks are also criticized for consuming a lot of memory and draining battery life of devices during training and inference. This makes it hard to deploy these models on mobile or embedded devices which have tight resource constraints. Quantization is recognized as one of the most effective approaches to satisfy the extreme memory requirements that deep neural network models demand. Instead of adopting 32-bit floating point format to represent weights, quantized representations store weights using more compact formats such as integers or even binary numbers. Despite a possible degradation in predictive performance, quantization provides a potential solution to greatly reduce the model size and the energy consumption. In this survey, we give a thorough review of different aspects of quantized neural networks. Current challenges and trends of quantized neural networks are also discussed.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {FOS: Computer and information sciences,Machine Learning (cs.LG),Machine Learning (stat.ML),Neural and Evolutionary Computing (cs.NE)}
}

@incollection{guReviewResearchNetworkonChip2011,
  title = {A {{Review}} of {{Research}} on {{Network-on-Chip Simulator}}},
  booktitle = {Communication {{Systems}} and {{Information Technology}}},
  author = {Gu, Haiyun},
  editor = {Ma, Ming},
  year = {2011},
  volume = {100},
  pages = {103--110},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-21762-3_13},
  isbn = {978-3-642-21761-6 978-3-642-21762-3}
}

@inbook{haj-yahyaStaticPowerModeling2018,
  title = {Static {{Power Modeling}} for {{Modern Processor}}},
  booktitle = {Energy {{Efficient High Performance Processors}}},
  author = {{Haj-Yahya}, Jawad and Mendelson, Avi and Ben Asher, Yosi and Chattopadhyay, Anupam},
  year = {2018},
  pages = {135--165},
  publisher = {Springer Singapore},
  address = {Singapore},
  doi = {10.1007/978-981-10-8554-3_5},
  collaborator = {{Haj-Yahya}, Jawad and Mendelson, Avi and Ben Asher, Yosi and Chattopadhyay, Anupam},
  isbn = {978-981-10-8553-6 978-981-10-8554-3}
}

@misc{heDeepResidualLearning2015,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = {2015},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.1512.03385},
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \&amp; COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences}
}

@book{hennessyComputerArchitectureSixth2017,
  title = {Computer {{Architecture}}, {{Sixth Edition}}: {{A Quantitative Approach}}},
  author = {Hennessy, John L. and Patterson, David A.},
  year = {2017},
  edition = {6th},
  publisher = {Morgan Kaufmann Publishers Inc.},
  address = {San Francisco, CA, USA},
  abstract = {Computer Architecture: A Quantitative Approach, Sixth Edition has been considered essential reading by instructors, students and practitioners of computer design for over 20 years. The sixth edition of this classic textbook is fully revised with the latest developments in processor and system architecture. It now features examples from the RISC-V (RISC Five) instruction set architecture, a modern RISC instruction set developed and designed to be a free and openly adoptable standard. It also includes a new chapter on domain-specific architectures and an updated chapter on warehouse-scale computing that features the first public information on Google's newest WSC. True to its original mission of demystifying computer architecture, this edition continues the longstanding tradition of focusing on areas where the most exciting computing innovation is happening, while always keeping an emphasis on good engineering design. Includes a new chapter on domain-specific architectures, explaining how they are the only path forward for improved performance and energy efficiency given the end of Moores Law and Dennard scaling Features the first publication of several DSAs from industry Features extensive updates to the chapter on warehouse-scale computing, with the first public information on the newest Google WSC Offers updates to other chapters including new material dealing with the use of stacked DRAM; data on the performance of new NVIDIA Pascal GPU vs. new AVX-512 Intel Skylake CPU; and extensive additions to content covering multicore architecture and organization Includes "Putting It All Together" sections near the end of every chapter, providing real-world technology examples that demonstrate the principles covered in each chapter Includes review appendices in the printed text and additional reference appendices available online Includes updated and improved case studies and exercises},
  isbn = {0-12-811905-5}
}

@inproceedings{huangSwapAdvisorPushingDeep2020,
  title = {{{SwapAdvisor}}: {{Pushing Deep Learning Beyond}} the {{GPU Memory Limit}} via {{Smart Swapping}}},
  shorttitle = {{{SwapAdvisor}}},
  booktitle = {Proceedings of the {{Twenty-Fifth International Conference}} on {{Architectural Support}} for {{Programming Languages}} and {{Operating Systems}}},
  author = {Huang, Chien-Chin and Jin, Gu and Li, Jinyang},
  year = {2020},
  month = mar,
  pages = {1341--1355},
  publisher = {ACM},
  address = {Lausanne Switzerland},
  doi = {10.1145/3373376.3378530},
  isbn = {978-1-4503-7102-5}
}

@article{indiveriMemoryInformationProcessing2015,
  title = {Memory and {{Information Processing}} in {{Neuromorphic Systems}}},
  author = {Indiveri, Giacomo and Liu, Shih-Chii},
  year = {2015},
  month = aug,
  journal = {Proceedings of the IEEE},
  volume = {103},
  number = {8},
  pages = {1379--1397},
  issn = {0018-9219, 1558-2256},
  doi = {10.1109/JPROC.2015.2444094},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html}
}

@techreport{JEDEC_JESD209-5C,
  title = {Low {{Power Double Data Rate}} ({{LPDDR}}) 5/{{5X}}},
  author = {{JEDEC Solid State Technology Association}},
  year = {2023},
  month = jul,
  number = {JESD209-5C},
  institution = {JEDEC Solid State Technology Association}
}

@inproceedings{jiDemandLayeringRealTime2022,
  title = {Demand {{Layering}} for {{Real-Time DNN Inference}} with {{Minimized Memory Usage}}},
  booktitle = {2022 {{IEEE Real-Time Systems Symposium}} ({{RTSS}})},
  author = {Ji, Mingoo and Yi, Saehanseul and Koo, Changjin and Ahn, Sol and Seo, Dongjoo and Dutt, Nikil and Kim, Jong-Chan},
  year = {2022},
  month = dec,
  pages = {291--304},
  publisher = {IEEE},
  address = {Houston, TX, USA},
  doi = {10.1109/RTSS55097.2022.00033},
  copyright = {https://doi.org/10.15223/policy-029},
  isbn = {978-1-6654-5346-2}
}

@incollection{kaboubiHybridPartitioningEmbedded2023,
  title = {Hybrid {{Partitioning}} for {{Embedded}} and {{Distributed CNNs Inference}} on {{Edge Devices}}},
  booktitle = {Advanced {{Network Technologies}} and {{Intelligent Computing}}},
  author = {Kaboubi, Nihel and Letondeur, Lo{\"i}c and Coupaye, Thierry and Desprez, Fr{\'e}deric and Trystram, Denis},
  editor = {Woungang, Isaac and Dhurandher, Sanjay Kumar and Pattanaik, Kiran Kumar and Verma, Anshul and Verma, Pradeepika},
  year = {2023},
  volume = {1797},
  pages = {164--187},
  publisher = {Springer Nature Switzerland},
  address = {Cham},
  doi = {10.1007/978-3-031-28180-8_12},
  isbn = {978-3-031-28179-2 978-3-031-28180-8}
}

@inproceedings{kangLaLaRANDFlexibleLayerbyLayer2021,
  title = {{{LaLaRAND}}: {{Flexible Layer-by-Layer CPU}}/{{GPU Scheduling}} for {{Real-Time DNN Tasks}}},
  shorttitle = {{{LaLaRAND}}},
  booktitle = {2021 {{IEEE Real-Time Systems Symposium}} ({{RTSS}})},
  author = {Kang, Woosung and Lee, Kilho and Lee, Jinkyu and Shin, Insik and Chwa, Hoon Sung},
  year = {2021},
  month = dec,
  pages = {329--341},
  publisher = {IEEE},
  address = {Dortmund, DE},
  doi = {10.1109/RTSS52674.2021.00038},
  copyright = {https://doi.org/10.15223/policy-029},
  isbn = {978-1-6654-2802-6}
}

@misc{KerasDocumentationResNet,
  title = {Keras Documentation: {{ResNet}} and {{ResNetV2}}},
  shorttitle = {Keras Documentation},
  abstract = {Keras documentation},
  howpublished = {https://keras.io/api/applications/resnet/}
}

@article{khanComparativeAnalysisNetwork2018,
  title = {Comparative Analysis of Network-on-chip Simulation Tools},
  author = {Khan, Sarzamin and Anjum, Sheraz and Gulzari, Usman Ali and Torres, Frank Sill},
  year = {2018},
  month = jan,
  journal = {IET Computers \& Digital Techniques},
  volume = {12},
  number = {1},
  pages = {30--38},
  issn = {1751-8601, 1751-861X},
  doi = {10.1049/iet-cdt.2017.0068}
}

@phdthesis{killebrewL2CacheOffchip2008,
  title = {L2 {{Cache}} to {{Off-chip Memory Networks}} for {{Chip Multiprocessors}}},
  author = {Killebrew, Carrell D},
  year = {2008},
  month = may,
  school = {University of California at Berkeley}
}

@misc{krishnamoorthiQuantizingDeepConvolutional2018,
  title = {Quantizing Deep Convolutional Networks for Efficient Inference: {{A}} Whitepaper},
  shorttitle = {Quantizing Deep Convolutional Networks for Efficient Inference},
  author = {Krishnamoorthi, Raghuraman},
  year = {2018},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.1806.08342},
  abstract = {We present an overview of techniques for quantizing convolutional neural networks for inference with integer weights and activations. Per-channel quantization of weights and per-layer quantization of activations to 8-bits of precision post-training produces classification accuracies within 2\% of floating point networks for a wide variety of CNN architectures. Model sizes can be reduced by a factor of 4 by quantizing weights to 8-bits, even when 8-bit arithmetic is not supported. This can be achieved with simple, post training quantization of weights.We benchmark latencies of quantized networks on CPUs and DSPs and observe a speedup of 2x-3x for quantized implementations compared to floating point on CPUs. Speedups of up to 10x are observed on specialized processors with fixed point SIMD capabilities, like the Qualcomm QDSPs with HVX. Quantization-aware training can provide further improvements, reducing the gap to floating point to 1\% at 8-bit precision. Quantization-aware training also allows for reducing the precision of weights to four bits with accuracy losses ranging from 2\% to 10\%, with higher accuracy drop for smaller networks.We introduce tools in TensorFlow and TensorFlowLite for quantizing convolutional networks and review best practices for quantization-aware training to obtain high accuracy with quantized weights and activations. We recommend that per-channel quantization of weights and per-layer quantization of activations be the preferred quantization scheme for hardware acceleration and kernel optimization. We also propose that future processors and hardware accelerators for optimized inference support precisions of 4, 8 and 16 bits.},
  copyright = {Creative Commons Zero v1.0 Universal},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences,Machine Learning (cs.LG),Machine Learning (stat.ML)}
}

@article{krizhevskyImageNetClassificationDeep2017,
  title = {{{ImageNet}} Classification with Deep Convolutional Neural Networks},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
  year = {2017},
  month = may,
  journal = {Communications of the ACM},
  volume = {60},
  number = {6},
  pages = {84--90},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/3065386},
  abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.}
}

@article{lecunDeepLearning2015,
  title = {Deep Learning},
  author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  year = {2015},
  month = may,
  journal = {Nature},
  volume = {521},
  number = {7553},
  pages = {436--444},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature14539}
}

@article{leEfficientNeuralNetworks2023,
  title = {Efficient {{Neural Networks}} for {{Tiny Machine Learning}}: {{A Comprehensive Review}}},
  shorttitle = {Efficient {{Neural Networks}} for {{Tiny Machine Learning}}},
  author = {L{\^e}, Minh Tri and Wolinski, Pierre and Arbel, Julyan},
  year = {2023},
  journal = {arXiv},
  publisher = {[object Object]},
  doi = {10.48550/ARXIV.2311.11883},
  abstract = {The field of Tiny Machine Learning (TinyML) has gained significant attention due to its potential to enable intelligent applications on resource-constrained devices. This review provides an in-depth analysis of the advancements in efficient neural networks and the deployment of deep learning models on ultra-low power microcontrollers (MCUs) for TinyML applications. It begins by introducing neural networks and discussing their architectures and resource requirements. It then explores MEMS-based applications on ultra-low power MCUs, highlighting their potential for enabling TinyML on resource-constrained devices. The core of the review centres on efficient neural networks for TinyML. It covers techniques such as model compression, quantization, and low-rank factorization, which optimize neural network architectures for minimal resource utilization on MCUs. The paper then delves into the deployment of deep learning models on ultra-low power MCUs, addressing challenges such as limited computational capabilities and memory resources. Techniques like model pruning, hardware acceleration, and algorithm-architecture co-design are discussed as strategies to enable efficient deployment. Lastly, the review provides an overview of current limitations in the field, including the trade-off between model complexity and resource constraints. Overall, this review paper presents a comprehensive analysis of efficient neural networks and deployment strategies for TinyML on ultra-low-power MCUs. It identifies future research directions for unlocking the full potential of TinyML applications on resource-constrained devices.},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {Computation (stat.CO),FOS: Computer and information sciences,Machine Learning (cs.LG),Machine Learning (stat.ML)}
}

@inproceedings{liuEdgeAssistedRealtime2019,
  title = {Edge {{Assisted Real-time Object Detection}} for {{Mobile Augmented Reality}}},
  booktitle = {The 25th {{Annual International Conference}} on {{Mobile Computing}} and {{Networking}}},
  author = {Liu, Luyang and Li, Hongyu and Gruteser, Marco},
  year = {2019},
  month = aug,
  pages = {1--16},
  publisher = {ACM},
  address = {Los Cabos Mexico},
  doi = {10.1145/3300061.3300116},
  isbn = {978-1-4503-6169-9}
}

@inproceedings{liuEdgeEyeEdgeService2018,
  title = {{{EdgeEye}}: {{An Edge Service Framework}} for {{Real-time Intelligent Video Analytics}}},
  shorttitle = {{{EdgeEye}}},
  booktitle = {Proceedings of the 1st {{International Workshop}} on {{Edge Systems}}, {{Analytics}} and {{Networking}}},
  author = {Liu, Peng and Qi, Bozhao and Banerjee, Suman},
  year = {2018},
  month = jun,
  pages = {1--6},
  publisher = {ACM},
  address = {Munich Germany},
  doi = {10.1145/3213344.3213345},
  isbn = {978-1-4503-5837-8}
}

@inproceedings{manhokimNetworkonchipLinkAnalysis2006,
  title = {Network-on-Chip Link Analysis under Power and Performance Constraints},
  booktitle = {2006 {{IEEE International Symposium}} on {{Circuits}} and {{Systems}}},
  author = {{Manho Kim} and {Daewook Kim} and Sobelman, G.E.},
  year = {2006},
  pages = {4},
  publisher = {IEEE},
  address = {Island of Kos, Greece},
  doi = {10.1109/ISCAS.2006.1693546},
  isbn = {978-0-7803-9389-9}
}

@article{meenaOverviewEmergingNonvolatile2014,
  title = {Overview of Emerging Nonvolatile Memory Technologies},
  author = {Meena, Jagan Singh and Sze, Simon Min and Chand, Umesh and Tseng, Tseung-Yuen},
  year = {2014},
  month = dec,
  journal = {Nanoscale Research Letters},
  volume = {9},
  number = {1},
  pages = {526},
  issn = {1556-276X},
  doi = {10.1186/1556-276X-9-526},
  abstract = {Abstract             Nonvolatile memory technologies in Si-based electronics date back to the 1990s. Ferroelectric field-effect transistor (FeFET) was one of the most promising devices replacing the conventional Flash memory facing physical scaling limitations at those times. A variant of charge storage memory referred to as Flash memory is widely used in consumer electronic products such as cell phones and music players while NAND Flash-based solid-state disks (SSDs) are increasingly displacing hard disk drives as the primary storage device in laptops, desktops, and even data centers. The integration limit of Flash memories is approaching, and many new types of memory to replace conventional Flash memories have been proposed. Emerging memory technologies promise new memories to store more data at less cost than the expensive-to-build silicon chips used by popular consumer gadgets including digital cameras, cell phones and portable music players. They are being investigated and lead to the future as potential alternatives to existing memories in future computing systems. Emerging nonvolatile memory technologies such as magnetic random-access memory (MRAM), spin-transfer torque random-access memory (STT-RAM), ferroelectric random-access memory (FeRAM), phase-change memory (PCM), and resistive random-access memory (RRAM) combine the speed of static random-access memory (SRAM), the density of dynamic random-access memory (DRAM), and the nonvolatility of Flash memory and so become very attractive as another possibility for future memory hierarchies. Many other new classes of emerging memory technologies such as transparent and plastic, three-dimensional (3-D), and quantum dot memory technologies have also gained tremendous popularity in recent years. Subsequently, not an exaggeration to say that computer memory could soon earn the ultimate commercial validation for commercial scale-up and production the cheap plastic knockoff. Therefore, this review is devoted to the rapidly developing new class of memory technologies and scaling of scientific procedures based on an investigation of recent progress in advanced Flash memory devices.}
}

@article{miaoEnablingLargeNeural2021,
  title = {Enabling {{Large Neural Networks}} on {{Tiny Microcontrollers}} with {{Swapping}}},
  author = {Miao, Hongyu and Lin, Felix Xiaozhu},
  year = {2021},
  journal = {arXiv},
  publisher = {[object Object]},
  doi = {10.48550/ARXIV.2101.08744},
  abstract = {Running neural networks (NNs) on microcontroller units (MCUs) is becoming increasingly important, but is very difficult due to the tiny SRAM size of MCU. Prior work proposes many algorithm-level techniques to reduce NN memory footprints, but all at the cost of sacrificing accuracy and generality, which disqualifies MCUs for many important use cases. We investigate a system solution for MCUs to execute NNs out of core: dynamically swapping NN data chunks between an MCU's tiny SRAM and its large, low-cost external flash. Out-of-core NNs on MCUs raise multiple concerns: execution slowdown, storage wear out, energy consumption, and data security. We present a study showing that none is a showstopper; the key benefit -- MCUs being able to run large NNs with full accuracy and generality -- triumphs the overheads. Our findings suggest that MCUs can play a much greater role in edge intelligence.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {FOS: Computer and information sciences,Hardware Architecture (cs.AR),Operating Systems (cs.OS)}
}

@inproceedings{moreiraNeuronFlowNeuromorphicProcessor2020,
  title = {{{NeuronFlow}}: A Neuromorphic Processor Architecture for {{Live AI}} Applications},
  shorttitle = {{{NeuronFlow}}},
  booktitle = {2020 {{Design}}, {{Automation}} \& {{Test}} in {{Europe Conference}} \& {{Exhibition}} ({{DATE}})},
  author = {Moreira, Orlando and Yousefzadeh, Amirreza and Chersi, Fabian and Cinserin, Gokturk and Zwartenkot, Rik-Jan and Kapoor, Ajay and Qiao, Peng and Kievits, Peter and Khoei, Mina and Rouillard, Louis and Ferouge, Aimee and Tapson, Jonathan and Visweswara, Ashoka},
  year = {2020},
  month = mar,
  pages = {840--845},
  publisher = {IEEE},
  address = {Grenoble, France},
  doi = {10.23919/DATE48585.2020.9116352},
  isbn = {978-3-9819263-4-7}
}

@misc{neillOverviewNeuralNetwork2020,
  title = {An {{Overview}} of {{Neural Network Compression}}},
  author = {Neill, James O'},
  year = {2020},
  month = aug,
  number = {arXiv:2006.03669},
  eprint = {2006.03669},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  abstract = {Overparameterized networks trained to convergence have shown impressive performance in domains such as computer vision and natural language processing. Pushing state of the art on salient tasks within these domains corresponds to these models becoming larger and more difficult for machine learning practitioners to use given the increasing memory and storage requirements, not to mention the larger carbon footprint. Thus, in recent years there has been a resurgence in model compression techniques, particularly for deep convolutional neural networks and self-attention based networks such as the Transformer. Hence, this paper provides a timely overview of both old and current compression techniques for deep neural networks, including pruning, quantization, tensor decomposition, knowledge distillation and combinations thereof. We assume a basic familiarity with deep learning architectures{\textbackslash}footnote\{For an introduction to deep learning, see {\textasciitilde}{\textbackslash}citet\{goodfellow2016deep\}\}, namely, Recurrent Neural Networks{\textasciitilde}{\textbackslash}citep[(RNNs)][]\{rumelhart1985learning,hochreiter1997long\}, Convolutional Neural Networks{\textasciitilde}{\textbackslash}citep\{fukushima1980neocognitron\}{\textasciitilde}{\textbackslash}footnote\{For an up to date overview see{\textasciitilde}{\textbackslash}citet\{khan2019survey\}\} and Self-Attention based networks{\textasciitilde}{\textbackslash}citep\{vaswani2017attention\}{\textbackslash}footnote\{For a general overview of self-attention networks, see {\textasciitilde}{\textbackslash}citet\{chaudhari2019attentive\}.\},{\textbackslash}footnote\{For more detail and their use in natural language processing, see{\textasciitilde}{\textbackslash}citet\{hu2019introductory\}\}. Most of the papers discussed are proposed in the context of at least one of these DNN architectures.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@inproceedings{oExploringEnergyefficientDRAM2011,
  title = {Exploring Energy-Efficient {{DRAM}} Array Organizations},
  booktitle = {2011 {{IEEE}} 54th {{International Midwest Symposium}} on {{Circuits}} and {{Systems}} ({{MWSCAS}})},
  author = {O, Seongil and Choo, Sungwoo and Ahn, Jung Ho},
  year = {2011},
  month = aug,
  pages = {1--4},
  publisher = {IEEE},
  address = {Seoul, Korea (South)},
  doi = {10.1109/MWSCAS.2011.6026486},
  isbn = {978-1-61284-856-3}
}

@article{painkrasSpiNNaker1W18Core2013,
  title = {{{SpiNNaker}}: {{A}} 1-{{W}} 18-{{Core System-on-Chip}} for {{Massively-Parallel Neural Network Simulation}}},
  shorttitle = {{{SpiNNaker}}},
  author = {Painkras, Eustace and Plana, Luis A. and Garside, Jim and Temple, Steve and Galluppi, Francesco and Patterson, Cameron and Lester, David R. and Brown, Andrew D. and Furber, Steve B.},
  year = {2013},
  month = aug,
  journal = {IEEE Journal of Solid-State Circuits},
  volume = {48},
  number = {8},
  pages = {1943--1953},
  issn = {0018-9200, 1558-173X},
  doi = {10.1109/JSSC.2013.2259038},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html}
}

@article{paramasivamNETWORKONCHIPITS2015,
  title = {{{NETWORK ON-CHIP AND ITS RESEARCH CHALLENGES}}},
  author = {Paramasivam, K.},
  year = {2015},
  month = jul,
  journal = {ICTACT Journal on Microelectronics},
  volume = {01},
  number = {02},
  pages = {83--87},
  issn = {23951672, 23951680},
  doi = {10.21917/ijme.2015.0015}
}

@inproceedings{parikhPowerAwareNoCsRouting2014,
  title = {Power-{{Aware NoCs}} through {{Routing}} and {{Topology Reconfiguration}}},
  booktitle = {Proceedings of the 51st {{Annual Design Automation Conference}}},
  author = {Parikh, Ritesh and Das, Reetuparna and Bertacco, Valeria},
  year = {2014},
  month = jun,
  pages = {1--6},
  publisher = {ACM},
  address = {San Francisco CA USA},
  doi = {10.1145/2593069.2593187},
  isbn = {978-1-4503-2730-5}
}

@misc{pascanuDifficultyTrainingRecurrent2012,
  title = {On the Difficulty of Training {{Recurrent Neural Networks}}},
  author = {Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
  year = {2012},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.1211.5063},
  abstract = {There are two widely known issues with properly training Recurrent Neural Networks, the vanishing and the exploding gradient problems detailed in Bengio et al. (1994). In this paper we attempt to improve the understanding of the underlying issues by exploring these problems from an analytical, a geometric and a dynamical systems perspective. Our analysis is used to justify a simple yet effective solution. We propose a gradient norm clipping strategy to deal with exploding gradients and a soft constraint for the vanishing gradients problem. We validate empirically our hypothesis and proposed solutions in the experimental section.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {FOS: Computer and information sciences,Machine Learning (cs.LG)}
}

@inproceedings{ranDeepDecisionMobileDeep2018,
  title = {{{DeepDecision}}: {{A Mobile Deep Learning Framework}} for {{Edge Video Analytics}}},
  shorttitle = {{{DeepDecision}}},
  booktitle = {{{IEEE INFOCOM}} 2018 - {{IEEE Conference}} on {{Computer Communications}}},
  author = {Ran, Xukan and Chen, Haolianz and Zhu, Xiaodan and Liu, Zhenming and Chen, Jiasi},
  year = {2018},
  month = apr,
  pages = {1421--1429},
  publisher = {IEEE},
  address = {Honolulu, HI},
  doi = {10.1109/INFOCOM.2018.8485905},
  isbn = {978-1-5386-4128-6}
}

@inproceedings{requenaEfficientSwitchingTechnique2008,
  title = {An {{Efficient Switching Technique}} for {{NoCs}} with {{Reduced Buffer Requirements}}},
  booktitle = {2008 14th {{IEEE International Conference}} on {{Parallel}} and {{Distributed Systems}}},
  author = {Requena, Crisp{\'i}n G{\'o}mez and Requena, Mar{\'i}a Engracia G{\'o}mez and Rodr{\'i}guez, Pedro Juan L{\'o}pez and Mar{\'i}n, Jose Duato},
  year = {2008},
  month = dec,
  pages = {713--720},
  publisher = {IEEE},
  address = {Melbourne, VIC},
  doi = {10.1109/ICPADS.2008.43},
  isbn = {978-0-7695-3434-3}
}

@article{rumelhartLearningRepresentationsBackpropagating1986,
  title = {Learning Representations by Back-Propagating Errors},
  author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
  year = {1986},
  month = oct,
  journal = {Nature},
  volume = {323},
  number = {6088},
  pages = {533--536},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/323533a0},
  copyright = {http://www.springer.com/tdm}
}

@misc{russakovskyImageNetLargeScale2014,
  title = {{{ImageNet Large Scale Visual Recognition Challenge}}},
  author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and {Fei-Fei}, Li},
  year = {2014},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.1409.0575},
  abstract = {The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the five years of the challenge, and propose future directions and improvements.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences,I.4.8; I.5.2}
}

@article{seemaNetworkonChipStateoftheartReview2017,
  title = {Network-on-{{Chip}}: {{A State-of-the-art Review}}},
  shorttitle = {Network-on-{{Chip}}},
  author = {Seema, Seema and Dahiya, Pawan Kumar},
  year = {2017},
  month = jul,
  journal = {IOSR Journal of VLSI and Signal Processing},
  volume = {07},
  number = {04},
  pages = {29--35},
  issn = {23194197, 23194200},
  doi = {10.9790/4200-0704012935}
}

@misc{tensorflowdevelopersTensorFlow2024,
  title = {{{TensorFlow}}},
  author = {TensorFlow Developers},
  year = {2024},
  month = oct,
  doi = {10.5281/ZENODO.4724125},
  abstract = {TensorFlow is an end-to-end open source platform for machine learning. It has a comprehensive, flexible ecosystem of tools, libraries, and community resources that lets researchers push the state-of-the-art in ML and developers easily build and deploy ML-powered applications.},
  copyright = {Apache License 2.0},
  howpublished = {Zenodo}
}

@misc{TfkerasapplicationsResNet101TensorFlowV2161,
  title = {Tf.Keras.Applications.{{ResNet101}} {\textbar} {{TensorFlow}} v2.16.1},
  journal = {TensorFlow},
  abstract = {Instantiates the ResNet101 architecture.},
  howpublished = {https://www.tensorflow.org/api\_docs/python/tf/keras/applications/ResNet101}
}

@inproceedings{thanh-vule-vanSimulationPerformanceEvaluation2012,
  title = {Simulation and Performance Evaluation of a {{Network-on-Chip}} Architecture Based on {{SystemC}}},
  booktitle = {The 2012 {{International Conference}} on {{Advanced Technologies}} for {{Communications}}},
  author = {{Thanh-Vu Le-Van} and {Xuan-Tu Tran} and {Dien-Tap Ngo}},
  year = {2012},
  month = oct,
  pages = {170--175},
  publisher = {IEEE},
  address = {Ha Noi, Vietnam},
  doi = {10.1109/ATC.2012.6404252},
  isbn = {978-1-4673-4352-7 978-1-4673-4351-0}
}

@article{udipiRethinkingDRAMDesign2010,
  title = {Rethinking {{DRAM}} Design and Organization for Energy-Constrained Multi-Cores},
  author = {Udipi, Aniruddha N. and Muralimanohar, Naveen and Chatterjee, Niladrish and Balasubramonian, Rajeev and Davis, Al and Jouppi, Norman P.},
  year = {2010},
  month = jun,
  journal = {ACM SIGARCH Computer Architecture News},
  volume = {38},
  number = {3},
  pages = {175--186},
  issn = {0163-5964},
  doi = {10.1145/1816038.1815983},
  abstract = {DRAM vendors have traditionally optimized the cost-per-bit metric, often making design decisions that incur energy penalties. A prime example is the overfetch feature in DRAM, where a single request activates thousands of bit-lines in many DRAM chips, only to return a single cache line to the CPU. The focus on cost-per-bit is questionable in modern-day servers where operating costs can easily exceed the purchase cost. Modern technology trends are also placing very different demands on the memory system: (i)queuing delays are a significant component of memory access time, (ii) there is a high energy premium for the level of reliability expected for business-critical computing, and (iii) the memory access stream emerging from multi-core systems exhibits limited locality. All of these trends necessitate an overhaul of DRAM architecture, even if it means a slight compromise in the cost-per-bit metric.             This paper examines three primary innovations. The first is a modification to DRAM chip microarchitecture that re tains the traditional DDRx SDRAMinterface. Selective Bit-line Activation (SBA) waits for both RAS (row address) and CAS (column address) signals to arrive before activating exactly those bitlines that provide the requested cache line. SBA reduces energy consumption while incurring slight area and performance penalties. The second innovation, Single Subarray Access (SSA), fundamentally re-organizes the layout of DRAM arrays and the mapping of data to these arrays so that an entire cache line is fetched from a single subarray. It requires a different interface to the memory controller, reduces dynamic and background energy (by about 6X), incurs a slight area penalty (4\%), and can even lead to performance improvements (54\% on average) by reducing queuing delays. The third innovation further penalizes the cost-per-bit metric by adding a checksum feature to each cache line. This checksum error-detection feature can then be used to build stronger RAID-like fault tolerance, including chipkill-level reliability. Such a technique is especially crucial for the SSA architecture where the entire cache line is localized to a single chip. This DRAM chip microarchitectural change leads to a dramatic reduction in the energy and storage overheads for reliability. The proposed architectures will also apply to other emerging memory technologies (such as resistive memories) and will be less disruptive to standards, interfaces, and the design flow if they can be incorporated into first-generation designs.}
}

@inproceedings{wangExploringHybridMemory2013,
  title = {Exploring Hybrid Memory for {{GPU}} Energy Efficiency through Software-Hardware Co-Design},
  booktitle = {Proceedings of the 22nd {{International Conference}} on {{Parallel Architectures}} and {{Compilation Techniques}}},
  author = {Wang, Bin and Wu, Bo and Li, Dong and Shen, Xipeng and Yu, Weikuan and Jiao, Yizheng and Vetter, Jeffrey S.},
  year = {2013},
  month = sep,
  pages = {93--102},
  publisher = {IEEE},
  address = {Edinburgh},
  doi = {10.1109/PACT.2013.6618807},
  isbn = {978-1-4799-1018-2 978-1-4799-1021-2}
}

@article{wangSwapNetEfficientSwapping2024,
  title = {{{SwapNet}}: {{Efficient Swapping}} for {{DNN Inference}} on {{Edge AI Devices Beyond}} the {{Memory Budget}}},
  shorttitle = {{{SwapNet}}},
  author = {Wang, Kun and Cao, Jiani and Zhou, Zimu and Li, Zhenjiang},
  year = {2024},
  journal = {IEEE Transactions on Mobile Computing},
  pages = {1--14},
  issn = {1536-1233, 1558-0660, 2161-9875},
  doi = {10.1109/TMC.2024.3355764},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html}
}

@article{wangYOLOv7TrainableBagoffreebies2022,
  title = {{{YOLOv7}}: {{Trainable}} Bag-of-Freebies Sets New State-of-the-Art for Real-Time Object Detectors},
  shorttitle = {{{YOLOv7}}},
  author = {Wang, Chien-Yao and Bochkovskiy, Alexey and Liao, Hong-Yuan Mark},
  year = {2022},
  journal = {arXiv},
  publisher = {[object Object]},
  doi = {10.48550/ARXIV.2207.02696},
  abstract = {YOLOv7 surpasses all known object detectors in both speed and accuracy in the range from 5 FPS to 160 FPS and has the highest accuracy 56.8\% AP among all known real-time object detectors with 30 FPS or higher on GPU V100. YOLOv7-E6 object detector (56 FPS V100, 55.9\% AP) outperforms both transformer-based detector SWIN-L Cascade-Mask R-CNN (9.2 FPS A100, 53.9\% AP) by 509\% in speed and 2\% in accuracy, and convolutional-based detector ConvNeXt-XL Cascade-Mask R-CNN (8.6 FPS A100, 55.2\% AP) by 551\% in speed and 0.7\% AP in accuracy, as well as YOLOv7 outperforms: YOLOR, YOLOX, Scaled-YOLOv4, YOLOv5, DETR, Deformable DETR, DINO-5scale-R50, ViT-Adapter-B and many other object detectors in speed and accuracy. Moreover, we train YOLOv7 only on MS COCO dataset from scratch without using any other datasets or pre-trained weights. Source code is released in https://github.com/WongKinYiu/yolov7.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences}
}

@article{wentzlaffOnChipInterconnectionArchitecture2007,
  title = {On-{{Chip Interconnection Architecture}} of the {{Tile Processor}}},
  author = {Wentzlaff, D. and Griffin, P. and Hoffmann, H. and {Liewei Bao} and Edwards, B. and Ramey, C. and Mattina, M. and {Chyi-Chang Miao} and Brown, J.F. and Agarwal, A.},
  year = {2007},
  month = sep,
  journal = {IEEE Micro},
  volume = {27},
  number = {5},
  pages = {15--31},
  issn = {0272-1732},
  doi = {10.1109/MM.2007.4378780}
}

@inproceedings{wolkotteEnergyEfficientReconfigurableCircuitSwitched2005,
  title = {An {{Energy-Efficient Reconfigurable Circuit-Switched Network-on-Chip}}},
  booktitle = {19th {{IEEE International Parallel}} and {{Distributed Processing Symposium}}},
  author = {Wolkotte, P.T. and Smit, G.J.M. and Rauwerda, G.K. and Smit, L.T.},
  year = {2005},
  pages = {155a-155a},
  publisher = {IEEE},
  address = {Denver, CO, USA},
  doi = {10.1109/IPDPS.2005.95},
  isbn = {978-0-7695-2312-5}
}

@article{xuDeepWearAdaptiveLocal2020,
  title = {{{DeepWear}}: {{Adaptive Local Offloading}} for {{On-Wearable Deep Learning}}},
  shorttitle = {{{DeepWear}}},
  author = {Xu, Mengwei and Qian, Feng and Zhu, Mengze and Huang, Feifan and Pushp, Saumay and Liu, Xuanzhe},
  year = {2020},
  month = feb,
  journal = {IEEE Transactions on Mobile Computing},
  volume = {19},
  number = {2},
  pages = {314--330},
  issn = {1536-1233, 1558-0660, 2161-9875},
  doi = {10.1109/TMC.2019.2893250},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html}
}

@inproceedings{xuOptimalMemoryController2011,
  title = {Optimal Memory Controller Placement for Chip Multiprocessor},
  booktitle = {Proceedings of the Seventh {{IEEE}}/{{ACM}}/{{IFIP}} International Conference on {{Hardware}}/Software Codesign and System Synthesis},
  author = {Xu, Thomas Canhao and Liljeberg, Pasi and Tenhunen, Hannu},
  year = {2011},
  month = oct,
  pages = {217--226},
  publisher = {ACM},
  address = {Taipei Taiwan},
  doi = {10.1145/2039370.2039405},
  isbn = {978-1-4503-0715-4}
}

@inproceedings{yangExploitingPathDiversity2012,
  title = {Exploiting Path Diversity for Low-Latency and High-Bandwidth with the Dual-Path {{NoC}} Router},
  booktitle = {2012 {{IEEE International Symposium}} on {{Circuits}} and {{Systems}}},
  author = {Yang, Yoon Seok and Deshpande, Hrishikesh and Choi, Gwan and Gratz, Paul},
  year = {2012},
  month = may,
  pages = {2433--2436},
  publisher = {IEEE},
  address = {Seoul, Korea (South)},
  doi = {10.1109/ISCAS.2012.6271790},
  isbn = {978-1-4673-0219-7 978-1-4673-0218-0 978-1-4673-0217-3}
}

@article{yuIntelligentEdgeLeveraging2020,
  title = {Intelligent {{Edge}}: {{Leveraging Deep Imitation Learning}} for {{Mobile Edge Computation Offloading}}},
  shorttitle = {Intelligent {{Edge}}},
  author = {Yu, Shuai and Chen, Xu and Yang, Lei and Wu, Di and Bennis, Mehdi and Zhang, Junshan},
  year = {2020},
  month = feb,
  journal = {IEEE Wireless Communications},
  volume = {27},
  number = {1},
  pages = {92--99},
  issn = {1536-1284, 1558-0687},
  doi = {10.1109/MWC.001.1900232},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html}
}

@phdthesis{zhouPerformanceEvaluationNetworkonchip2009,
  title = {Performance Evaluation of Network-on-Chip Interconnect Architectures},
  author = {Zhou, Xinan},
  year = {2009},
  month = aug,
  school = {University of Nevada Las Vegas}
}

@inproceedings{ziaHighlyscalable3DCLOS2010,
  title = {Highly-Scalable {{3D CLOS NOC}} for Many-Core {{CMPs}}},
  booktitle = {Proceedings of the 8th {{IEEE International NEWCAS Conference}} 2010},
  author = {Zia, Aamir and Kannan, Sachhidh and Rose, Garrett and Chao, H. Jonathan},
  year = {2010},
  month = jun,
  pages = {229--232},
  publisher = {IEEE},
  address = {Montreal, QC, Canada},
  doi = {10.1109/NEWCAS.2010.5603776},
  isbn = {978-1-4244-6806-5}
}
