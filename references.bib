
@misc{russakovsky_imagenet_2014,
	title = {{ImageNet} Large Scale Visual Recognition Challenge},
	rights = {{arXiv}.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1409.0575},
	doi = {10.48550/ARXIV.1409.0575},
	abstract = {The {ImageNet} Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the five years of the challenge, and propose future directions and improvements.},
	publisher = {{arXiv}},
	author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
	urldate = {2024-10-09},
	date = {2014},
	note = {Version Number: 3},
	keywords = {Computer Vision and Pattern Recognition (cs.{CV}), {FOS}: Computer and information sciences, I.4.8; I.5.2},
}

@misc{he_deep_2015,
	title = {Deep Residual Learning for Image Recognition},
	rights = {{arXiv}.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1512.03385},
	doi = {10.48550/ARXIV.1512.03385},
	abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the {ImageNet} dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than {VGG} nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the {ImageNet} test set. This result won the 1st place on the {ILSVRC} 2015 classification task. We also present analysis on {CIFAR}-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the {COCO} object detection dataset. Deep residual nets are foundations of our submissions to {ILSVRC} \&amp; {COCO} 2015 competitions, where we also won the 1st places on the tasks of {ImageNet} detection, {ImageNet} localization, {COCO} detection, and {COCO} segmentation.},
	publisher = {{arXiv}},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	urldate = {2024-10-09},
	date = {2015},
	note = {Version Number: 1},
	keywords = {Computer Vision and Pattern Recognition (cs.{CV}), {FOS}: Computer and information sciences},
}

@online{noauthor_keras_nodate,
	title = {Keras documentation: {ResNet} and {ResNetV}2},
	url = {https://keras.io/api/applications/resnet/},
	shorttitle = {Keras documentation},
	abstract = {Keras documentation},
	urldate = {2024-10-09},
	langid = {english},
}

@online{noauthor_tfkerasapplicationsresnet101_nodate,
	title = {tf.keras.applications.{ResNet}101 {\textbar} {TensorFlow} v2.16.1},
	url = {https://www.tensorflow.org/api_docs/python/tf/keras/applications/ResNet101},
	abstract = {Instantiates the {ResNet}101 architecture.},
	titleaddon = {{TensorFlow}},
	urldate = {2024-10-09},
	langid = {english},
}

@inproceedings{glass_turn_1992,
	location = {Gold Coast, Australia},
	title = {The Turn Model for Adaptive Routing},
	isbn = {978-0-89791-509-0},
	url = {http://ieeexplore.ieee.org/document/753324/},
	doi = {10.1109/ISCA.1992.753324},
	eventtitle = {[1992] 19th Annual International Symposium on Computer Architecture},
	pages = {278--287},
	booktitle = {[1992] Proceedings the 19th Annual International Symposium on Computer Architecture},
	publisher = {{IEEE}},
	author = {Glass, C.J. and Ni, L.M.},
	urldate = {2024-06-11},
	date = {1992},
}

@inproceedings{ji_demand_2022,
	location = {Houston, {TX}, {USA}},
	title = {Demand Layering for Real-Time {DNN} Inference with Minimized Memory Usage},
	rights = {https://doi.org/10.15223/policy-029},
	isbn = {978-1-66545-346-2},
	url = {https://ieeexplore.ieee.org/document/9984745/},
	doi = {10.1109/RTSS55097.2022.00033},
	eventtitle = {2022 {IEEE} Real-Time Systems Symposium ({RTSS})},
	pages = {291--304},
	booktitle = {2022 {IEEE} Real-Time Systems Symposium ({RTSS})},
	publisher = {{IEEE}},
	author = {Ji, Mingoo and Yi, Saehanseul and Koo, Changjin and Ahn, Sol and Seo, Dongjoo and Dutt, Nikil and Kim, Jong-Chan},
	urldate = {2024-06-04},
	date = {2022-12},
}

@inproceedings{kang_lalarand_2021,
	location = {Dortmund, {DE}},
	title = {{LaLaRAND}: Flexible Layer-by-Layer {CPU}/{GPU} Scheduling for Real-Time {DNN} Tasks},
	rights = {https://doi.org/10.15223/policy-029},
	isbn = {978-1-66542-802-6},
	url = {https://ieeexplore.ieee.org/document/9622325/},
	doi = {10.1109/RTSS52674.2021.00038},
	shorttitle = {{LaLaRAND}},
	eventtitle = {2021 {IEEE} Real-Time Systems Symposium ({RTSS})},
	pages = {329--341},
	booktitle = {2021 {IEEE} Real-Time Systems Symposium ({RTSS})},
	publisher = {{IEEE}},
	author = {Kang, Woosung and Lee, Kilho and Lee, Jinkyu and Shin, Insik and Chwa, Hoon Sung},
	urldate = {2024-06-04},
	date = {2021-12},
}

@incollection{woungang_hybrid_2023,
	location = {Cham},
	title = {Hybrid Partitioning for Embedded and Distributed {CNNs} Inference on Edge Devices},
	volume = {1797},
	isbn = {978-3-031-28179-2 978-3-031-28180-8},
	url = {https://link.springer.com/10.1007/978-3-031-28180-8_12},
	pages = {164--187},
	booktitle = {Advanced Network Technologies and Intelligent Computing},
	publisher = {Springer Nature Switzerland},
	author = {Kaboubi, Nihel and Letondeur, Loïc and Coupaye, Thierry and Desprez, Fréderic and Trystram, Denis},
	editor = {Woungang, Isaac and Dhurandher, Sanjay Kumar and Pattanaik, Kiran Kumar and Verma, Anshul and Verma, Pradeepika},
	urldate = {2024-06-04},
	date = {2023},
	langid = {english},
	doi = {10.1007/978-3-031-28180-8_12},
	note = {Series Title: Communications in Computer and Information Science},
}

@article{indiveri_memory_2015,
	title = {Memory and Information Processing in Neuromorphic Systems},
	volume = {103},
	rights = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/{IEEE}.html},
	issn = {0018-9219, 1558-2256},
	url = {http://ieeexplore.ieee.org/document/7159144/},
	doi = {10.1109/JPROC.2015.2444094},
	pages = {1379--1397},
	number = {8},
	journaltitle = {Proceedings of the {IEEE}},
	shortjournal = {Proc. {IEEE}},
	author = {Indiveri, Giacomo and Liu, Shih-Chii},
	urldate = {2024-06-04},
	date = {2015-08},
}

@inproceedings{wang_exploring_2013,
	location = {Edinburgh},
	title = {Exploring hybrid memory for {GPU} energy efficiency through software-hardware co-design},
	isbn = {978-1-4799-1018-2 978-1-4799-1021-2},
	url = {https://ieeexplore.ieee.org/document/6618807/},
	doi = {10.1109/PACT.2013.6618807},
	eventtitle = {22nd International Conference on Parallel Architectures and Compilation Techniques ({PACT})},
	pages = {93--102},
	booktitle = {Proceedings of the 22nd International Conference on Parallel Architectures and Compilation Techniques},
	publisher = {{IEEE}},
	author = {Wang, Bin and Wu, Bo and Li, Dong and Shen, Xipeng and Yu, Weikuan and Jiao, Yizheng and Vetter, Jeffrey S.},
	urldate = {2024-06-03},
	date = {2013-09},
}

@article{meena_overview_2014,
	title = {Overview of emerging nonvolatile memory technologies},
	volume = {9},
	issn = {1556-276X},
	url = {https://link.springer.com/10.1186/1556-276X-9-526},
	doi = {10.1186/1556-276X-9-526},
	abstract = {Abstract
            Nonvolatile memory technologies in Si-based electronics date back to the 1990s. Ferroelectric field-effect transistor ({FeFET}) was one of the most promising devices replacing the conventional Flash memory facing physical scaling limitations at those times. A variant of charge storage memory referred to as Flash memory is widely used in consumer electronic products such as cell phones and music players while {NAND} Flash-based solid-state disks ({SSDs}) are increasingly displacing hard disk drives as the primary storage device in laptops, desktops, and even data centers. The integration limit of Flash memories is approaching, and many new types of memory to replace conventional Flash memories have been proposed. Emerging memory technologies promise new memories to store more data at less cost than the expensive-to-build silicon chips used by popular consumer gadgets including digital cameras, cell phones and portable music players. They are being investigated and lead to the future as potential alternatives to existing memories in future computing systems. Emerging nonvolatile memory technologies such as magnetic random-access memory ({MRAM}), spin-transfer torque random-access memory ({STT}-{RAM}), ferroelectric random-access memory ({FeRAM}), phase-change memory ({PCM}), and resistive random-access memory ({RRAM}) combine the speed of static random-access memory ({SRAM}), the density of dynamic random-access memory ({DRAM}), and the nonvolatility of Flash memory and so become very attractive as another possibility for future memory hierarchies. Many other new classes of emerging memory technologies such as transparent and plastic, three-dimensional (3-D), and quantum dot memory technologies have also gained tremendous popularity in recent years. Subsequently, not an exaggeration to say that computer memory could soon earn the ultimate commercial validation for commercial scale-up and production the cheap plastic knockoff. Therefore, this review is devoted to the rapidly developing new class of memory technologies and scaling of scientific procedures based on an investigation of recent progress in advanced Flash memory devices.},
	pages = {526},
	number = {1},
	journaltitle = {Nanoscale Research Letters},
	shortjournal = {Nanoscale Res Lett},
	author = {Meena, Jagan Singh and Sze, Simon Min and Chand, Umesh and Tseng, Tseung-Yuen},
	urldate = {2024-06-03},
	date = {2014-12},
	langid = {english},
}

@article{chen_increasing_2014,
	title = {Increasing off-chip bandwidth in multi-core processors with switchable pins},
	volume = {42},
	issn = {0163-5964},
	url = {https://dl.acm.org/doi/10.1145/2678373.2665730},
	doi = {10.1145/2678373.2665730},
	abstract = {Off-chip memory bandwidth has been considered as one of the major limiting factors to processor performance, especially for multi-cores and many-cores. Conventional processor design allocates a large portion of off-chip pins to deliver power, leaving a small number of pins for processor signal communication. We observed that the processor requires much less power than that can be supplied during memory intensive stages. This is due to the fact that the frequencies of processor cores waiting for data to be fetched from off-chip memories can be scaled down in order to save power without degrading performance. In this work, motivated by this observation, we propose a dynamic pin switch technique to alleviate the bandwidth limitation issue. The technique is introduced to dynamically exploit the surplus pins for power delivery in the memory intensive phases and uses them to provide extra bandwidth for the program executions, thus significantly boosting the performance},
	pages = {385--396},
	number = {3},
	journaltitle = {{ACM} {SIGARCH} Computer Architecture News},
	shortjournal = {{SIGARCH} Comput. Archit. News},
	author = {Chen, Shaoming and Hu, Yue and Zhang, Ying and Peng, Lu and Ardonne, Jesse and Irving, Samuel and Srivastava, Ashok},
	urldate = {2024-05-26},
	date = {2014-10-16},
	langid = {english},
}

@inproceedings{zia_highly-scalable_2010,
	location = {Montreal, {QC}, Canada},
	title = {Highly-scalable 3D {CLOS} {NOC} for many-core {CMPs}},
	isbn = {978-1-4244-6806-5},
	url = {http://ieeexplore.ieee.org/document/5603776/},
	doi = {10.1109/NEWCAS.2010.5603776},
	eventtitle = {2010 8th {IEEE} International {NEWCAS} Conference ({NEWCAS})},
	pages = {229--232},
	booktitle = {Proceedings of the 8th {IEEE} International {NEWCAS} Conference 2010},
	publisher = {{IEEE}},
	author = {Zia, Aamir and Kannan, Sachhidh and Rose, Garrett and Chao, H. Jonathan},
	urldate = {2024-05-26},
	date = {2010-06},
}

@article{udipi_rethinking_2010,
	title = {Rethinking {DRAM} design and organization for energy-constrained multi-cores},
	volume = {38},
	issn = {0163-5964},
	url = {https://dl.acm.org/doi/10.1145/1816038.1815983},
	doi = {10.1145/1816038.1815983},
	abstract = {{DRAM} vendors have traditionally optimized the cost-per-bit metric, often making design decisions that incur energy penalties. A prime example is the overfetch feature in {DRAM}, where a single request activates thousands of bit-lines in many {DRAM} chips, only to return a single cache line to the {CPU}. The focus on cost-per-bit is questionable in modern-day servers where operating costs can easily exceed the purchase cost. Modern technology trends are also placing very different demands on the memory system: (i)queuing delays are a significant component of memory access time, (ii) there is a high energy premium for the level of reliability expected for business-critical computing, and (iii) the memory access stream emerging from multi-core systems exhibits limited locality. All of these trends necessitate an overhaul of {DRAM} architecture, even if it means a slight compromise in the cost-per-bit metric.
            This paper examines three primary innovations. The first is a modification to {DRAM} chip microarchitecture that re tains the traditional {DDRx} {SDRAMinterface}. Selective Bit-line Activation ({SBA}) waits for both {RAS} (row address) and {CAS} (column address) signals to arrive before activating exactly those bitlines that provide the requested cache line. {SBA} reduces energy consumption while incurring slight area and performance penalties. The second innovation, Single Subarray Access ({SSA}), fundamentally re-organizes the layout of {DRAM} arrays and the mapping of data to these arrays so that an entire cache line is fetched from a single subarray. It requires a different interface to the memory controller, reduces dynamic and background energy (by about 6X), incurs a slight area penalty (4\%), and can even lead to performance improvements (54\% on average) by reducing queuing delays. The third innovation further penalizes the cost-per-bit metric by adding a checksum feature to each cache line. This checksum error-detection feature can then be used to build stronger {RAID}-like fault tolerance, including chipkill-level reliability. Such a technique is especially crucial for the {SSA} architecture where the entire cache line is localized to a single chip. This {DRAM} chip microarchitectural change leads to a dramatic reduction in the energy and storage overheads for reliability. The proposed architectures will also apply to other emerging memory technologies (such as resistive memories) and will be less disruptive to standards, interfaces, and the design flow if they can be incorporated into first-generation designs.},
	pages = {175--186},
	number = {3},
	journaltitle = {{ACM} {SIGARCH} Computer Architecture News},
	shortjournal = {{SIGARCH} Comput. Archit. News},
	author = {Udipi, Aniruddha N. and Muralimanohar, Naveen and Chatterjee, Niladrish and Balasubramonian, Rajeev and Davis, Al and Jouppi, Norman P.},
	urldate = {2024-05-26},
	date = {2010-06-19},
	langid = {english},
}

@article{bakhoda_designing_2013,
	title = {Designing on-chip networks for throughput accelerators},
	volume = {10},
	issn = {1544-3566, 1544-3973},
	url = {https://dl.acm.org/doi/10.1145/2512429},
	doi = {10.1145/2512429},
	abstract = {As the number of cores and threads in throughput accelerators such as Graphics Processing Units ({GPU}) increases, so does the importance of on-chip interconnection network design. This article explores throughput-effective Network-on-Chips ({NoC}) for future compute accelerators that employ Bulk-Synchronous Parallel ({BSP}) programming models such as {CUDA} and {OpenCL}. A hardware optimization is “throughput effective” if it improves parallel application-level performance per unit chip area. We evaluate performance of future looking workloads using detailed closed-loop simulations modeling compute nodes, {NoC}, and the {DRAM} memory system. We start from a mesh design with bisection bandwidth balanced to off-chip demand. Accelerator workloads tend to demand high off-chip memory bandwidth which results in a many-to-few traffic pattern when coupled with expected technology constraints of slow growth in pins-per-chip. Leveraging these observations we reduce {NoC} area by proposing a “checkerboard” {NoC} which alternates between conventional
              full
              routers and
              half
              routers with limited connectivity. Next, we show that increasing network terminal bandwidth at the nodes connected to {DRAM} controllers alleviates a significant fraction of the remaining imbalance resulting from the many-to-few traffic pattern. Furthermore, we propose a “double checkerboard inverted” {NoC} organization which takes advantage of channel slicing to reduce area while maintaining the performance improvements of the aforementioned techniques. This organization also has a simpler routing mechanism and improves average application throughput per unit area by 24.3\%.},
	pages = {1--35},
	number = {3},
	journaltitle = {{ACM} Transactions on Architecture and Code Optimization},
	shortjournal = {{ACM} Trans. Archit. Code Optim.},
	author = {Bakhoda, Ali and Kim, John and Aamodt, Tor M.},
	urldate = {2024-05-26},
	date = {2013-09-16},
	langid = {english},
}

@inproceedings{dulloor_system_2014,
	location = {Amsterdam The Netherlands},
	title = {System software for persistent memory},
	isbn = {978-1-4503-2704-6},
	url = {https://dl.acm.org/doi/10.1145/2592798.2592814},
	doi = {10.1145/2592798.2592814},
	eventtitle = {{EuroSys} 2014: Ninth Eurosys Conference 2014},
	pages = {1--15},
	booktitle = {Proceedings of the Ninth European Conference on Computer Systems},
	publisher = {{ACM}},
	author = {Dulloor, Subramanya R. and Kumar, Sanjay and Keshavamurthy, Anil and Lantz, Philip and Reddy, Dheeraj and Sankaran, Rajesh and Jackson, Jeff},
	urldate = {2024-05-26},
	date = {2014-04-14},
	langid = {english},
}

@inproceedings{o_exploring_2011,
	location = {Seoul, Korea (South)},
	title = {Exploring energy-efficient {DRAM} array organizations},
	isbn = {978-1-61284-856-3},
	url = {http://ieeexplore.ieee.org/document/6026486/},
	doi = {10.1109/MWSCAS.2011.6026486},
	eventtitle = {2011 {IEEE} 54th International Midwest Symposium on Circuits and Systems ({MWSCAS})},
	pages = {1--4},
	booktitle = {2011 {IEEE} 54th International Midwest Symposium on Circuits and Systems ({MWSCAS})},
	publisher = {{IEEE}},
	author = {O, Seongil and Choo, Sungwoo and Ahn, Jung Ho},
	urldate = {2024-05-26},
	date = {2011-08},
}

@inproceedings{manho_kim_network--chip_2006,
	location = {Island of Kos, Greece},
	title = {Network-on-chip link analysis under power and performance constraints},
	isbn = {978-0-7803-9389-9},
	url = {http://ieeexplore.ieee.org/document/1693546/},
	doi = {10.1109/ISCAS.2006.1693546},
	eventtitle = {2006 {IEEE} International Symposium on Circuits and Systems},
	pages = {4},
	booktitle = {2006 {IEEE} International Symposium on Circuits and Systems},
	publisher = {{IEEE}},
	author = {{Manho Kim} and {Daewook Kim} and Sobelman, G.E.},
	urldate = {2024-05-26},
	date = {2006},
}

@inproceedings{requena_efficient_2008,
	location = {Melbourne, {VIC}},
	title = {An Efficient Switching Technique for {NoCs} with Reduced Buffer Requirements},
	isbn = {978-0-7695-3434-3},
	url = {https://ieeexplore.ieee.org/document/4724384/},
	doi = {10.1109/ICPADS.2008.43},
	eventtitle = {2008 14th {IEEE} International Conference on Parallel and Distributed Systems},
	pages = {713--720},
	booktitle = {2008 14th {IEEE} International Conference on Parallel and Distributed Systems},
	publisher = {{IEEE}},
	author = {Requena, Crispín Gómez and Requena, María Engracia Gómez and Rodríguez, Pedro Juan López and Marín, Jose Duato},
	urldate = {2024-05-26},
	date = {2008-12},
}

@inproceedings{wolkotte_energy-efficient_2005,
	location = {Denver, {CO}, {USA}},
	title = {An Energy-Efficient Reconfigurable Circuit-Switched Network-on-Chip},
	isbn = {978-0-7695-2312-5},
	url = {http://ieeexplore.ieee.org/document/1420011/},
	doi = {10.1109/IPDPS.2005.95},
	eventtitle = {19th {IEEE} International Parallel and Distributed Processing Symposium},
	pages = {155a--155a},
	booktitle = {19th {IEEE} International Parallel and Distributed Processing Symposium},
	publisher = {{IEEE}},
	author = {Wolkotte, P.T. and Smit, G.J.M. and Rauwerda, G.K. and Smit, L.T.},
	urldate = {2024-05-26},
	date = {2005},
}

@inproceedings{parikh_power-aware_2014,
	location = {San Francisco {CA} {USA}},
	title = {Power-Aware {NoCs} through Routing and Topology Reconfiguration},
	isbn = {978-1-4503-2730-5},
	url = {https://dl.acm.org/doi/10.1145/2593069.2593187},
	doi = {10.1145/2593069.2593187},
	eventtitle = {{DAC} '14: The 51st Annual Design Automation Conference 2014},
	pages = {1--6},
	booktitle = {Proceedings of the 51st Annual Design Automation Conference},
	publisher = {{ACM}},
	author = {Parikh, Ritesh and Das, Reetuparna and Bertacco, Valeria},
	urldate = {2024-05-26},
	date = {2014-06},
	langid = {english},
}

@inproceedings{yang_exploiting_2012,
	location = {Seoul, Korea (South)},
	title = {Exploiting path diversity for low-latency and high-bandwidth with the dual-path {NoC} router},
	isbn = {978-1-4673-0219-7 978-1-4673-0218-0 978-1-4673-0217-3},
	url = {http://ieeexplore.ieee.org/document/6271790/},
	doi = {10.1109/ISCAS.2012.6271790},
	eventtitle = {2012 {IEEE} International Symposium on Circuits and Systems - {ISCAS} 2012},
	pages = {2433--2436},
	booktitle = {2012 {IEEE} International Symposium on Circuits and Systems},
	publisher = {{IEEE}},
	author = {Yang, Yoon Seok and Deshpande, Hrishikesh and Choi, Gwan and Gratz, Paul},
	urldate = {2024-05-26},
	date = {2012-05},
}

@article{furhad_extended_2015,
	title = {An Extended Diagonal Mesh Topology for Network-on-Chip Architectures},
	volume = {10},
	issn = {19750080},
	url = {http://gvpress.com/journals/IJMUE/vol10_no10/21.pdf},
	doi = {10.14257/ijmue.2015.10.10.21},
	pages = {197--210},
	number = {10},
	journaltitle = {International Journal of Multimedia and Ubiquitous Engineering},
	shortjournal = {{IJMUE}},
	author = {Furhad, Md. Hasan and Kim, Jong-Myon},
	urldate = {2024-05-26},
	date = {2015-10-31},
}

@inproceedings{chen_physical_2010,
	location = {Grenoble, France},
	title = {Physical vs. Virtual Express Topologies with Low-Swing Links for Future Many-Core {NoCs}},
	isbn = {978-1-4244-7085-3},
	url = {http://ieeexplore.ieee.org/document/5507548/},
	doi = {10.1109/NOCS.2010.26},
	eventtitle = {2010 Fourth {ACM}/{IEEE} International Symposium on Networks-on-Chip},
	pages = {173--180},
	booktitle = {2010 Fourth {ACM}/{IEEE} International Symposium on Networks-on-Chip},
	publisher = {{IEEE}},
	author = {Chen, Chia-Hsin Owen and Agarwal, Niket and Krishna, Tushar and Koo, Kyung-Hoae and Peh, Li-Shiuan and Saraswat, Krishna C.},
	urldate = {2024-05-26},
	date = {2010},
}

@article{wang_yolov7_2022,
	title = {{YOLOv}7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors},
	rights = {{arXiv}.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2207.02696},
	doi = {10.48550/ARXIV.2207.02696},
	shorttitle = {{YOLOv}7},
	abstract = {{YOLOv}7 surpasses all known object detectors in both speed and accuracy in the range from 5 {FPS} to 160 {FPS} and has the highest accuracy 56.8\% {AP} among all known real-time object detectors with 30 {FPS} or higher on {GPU} V100. {YOLOv}7-E6 object detector (56 {FPS} V100, 55.9\% {AP}) outperforms both transformer-based detector {SWIN}-L Cascade-Mask R-{CNN} (9.2 {FPS} A100, 53.9\% {AP}) by 509\% in speed and 2\% in accuracy, and convolutional-based detector {ConvNeXt}-{XL} Cascade-Mask R-{CNN} (8.6 {FPS} A100, 55.2\% {AP}) by 551\% in speed and 0.7\% {AP} in accuracy, as well as {YOLOv}7 outperforms: {YOLOR}, {YOLOX}, Scaled-{YOLOv}4, {YOLOv}5, {DETR}, Deformable {DETR}, {DINO}-5scale-R50, {ViT}-Adapter-B and many other object detectors in speed and accuracy. Moreover, we train {YOLOv}7 only on {MS} {COCO} dataset from scratch without using any other datasets or pre-trained weights. Source code is released in https://github.com/{WongKinYiu}/yolov7.},
	journaltitle = {{arXiv}},
	author = {Wang, Chien-Yao and Bochkovskiy, Alexey and Liao, Hong-Yuan Mark},
	urldate = {2024-04-04},
	date = {2022},
	note = {Publisher: [object Object]
Version Number: 1},
	keywords = {Computer Vision and Pattern Recognition (cs.{CV}), {FOS}: Computer and information sciences},
}

@article{miao_enabling_2021,
	title = {Enabling Large Neural Networks on Tiny Microcontrollers with Swapping},
	rights = {{arXiv}.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2101.08744},
	doi = {10.48550/ARXIV.2101.08744},
	abstract = {Running neural networks ({NNs}) on microcontroller units ({MCUs}) is becoming increasingly important, but is very difficult due to the tiny {SRAM} size of {MCU}. Prior work proposes many algorithm-level techniques to reduce {NN} memory footprints, but all at the cost of sacrificing accuracy and generality, which disqualifies {MCUs} for many important use cases. We investigate a system solution for {MCUs} to execute {NNs} out of core: dynamically swapping {NN} data chunks between an {MCU}'s tiny {SRAM} and its large, low-cost external flash. Out-of-core {NNs} on {MCUs} raise multiple concerns: execution slowdown, storage wear out, energy consumption, and data security. We present a study showing that none is a showstopper; the key benefit -- {MCUs} being able to run large {NNs} with full accuracy and generality -- triumphs the overheads. Our findings suggest that {MCUs} can play a much greater role in edge intelligence.},
	journaltitle = {{arXiv}},
	author = {Miao, Hongyu and Lin, Felix Xiaozhu},
	urldate = {2024-04-04},
	date = {2021},
	note = {Publisher: [object Object]
Version Number: 3},
	keywords = {{FOS}: Computer and information sciences, Hardware Architecture (cs.{AR}), Operating Systems (cs.{OS})},
}

@article{le_efficient_2023,
	title = {Efficient Neural Networks for Tiny Machine Learning: A Comprehensive Review},
	rights = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2311.11883},
	doi = {10.48550/ARXIV.2311.11883},
	shorttitle = {Efficient Neural Networks for Tiny Machine Learning},
	abstract = {The field of Tiny Machine Learning ({TinyML}) has gained significant attention due to its potential to enable intelligent applications on resource-constrained devices. This review provides an in-depth analysis of the advancements in efficient neural networks and the deployment of deep learning models on ultra-low power microcontrollers ({MCUs}) for {TinyML} applications. It begins by introducing neural networks and discussing their architectures and resource requirements. It then explores {MEMS}-based applications on ultra-low power {MCUs}, highlighting their potential for enabling {TinyML} on resource-constrained devices. The core of the review centres on efficient neural networks for {TinyML}. It covers techniques such as model compression, quantization, and low-rank factorization, which optimize neural network architectures for minimal resource utilization on {MCUs}. The paper then delves into the deployment of deep learning models on ultra-low power {MCUs}, addressing challenges such as limited computational capabilities and memory resources. Techniques like model pruning, hardware acceleration, and algorithm-architecture co-design are discussed as strategies to enable efficient deployment. Lastly, the review provides an overview of current limitations in the field, including the trade-off between model complexity and resource constraints. Overall, this review paper presents a comprehensive analysis of efficient neural networks and deployment strategies for {TinyML} on ultra-low-power {MCUs}. It identifies future research directions for unlocking the full potential of {TinyML} applications on resource-constrained devices.},
	journaltitle = {{arXiv}},
	author = {Lê, Minh Tri and Wolinski, Pierre and Arbel, Julyan},
	urldate = {2024-04-04},
	date = {2023},
	note = {Publisher: [object Object]
Version Number: 1},
	keywords = {Computation (stat.{CO}), {FOS}: Computer and information sciences, Machine Learning (cs.{LG}), Machine Learning (stat.{ML})},
}

@article{painkras_spinnaker_2013,
	title = {{SpiNNaker}: A 1-W 18-Core System-on-Chip for Massively-Parallel Neural Network Simulation},
	volume = {48},
	rights = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/{IEEE}.html},
	issn = {0018-9200, 1558-173X},
	url = {http://ieeexplore.ieee.org/document/6515159/},
	doi = {10.1109/JSSC.2013.2259038},
	shorttitle = {{SpiNNaker}},
	pages = {1943--1953},
	number = {8},
	journaltitle = {{IEEE} Journal of Solid-State Circuits},
	shortjournal = {{IEEE} J. Solid-State Circuits},
	author = {Painkras, Eustace and Plana, Luis A. and Garside, Jim and Temple, Steve and Galluppi, Francesco and Patterson, Cameron and Lester, David R. and Brown, Andrew D. and Furber, Steve B.},
	urldate = {2024-04-09},
	date = {2013-08},
}

@inproceedings{moreira_neuronflow_2020,
	location = {Grenoble, France},
	title = {{NeuronFlow}: a neuromorphic processor architecture for Live {AI} applications},
	isbn = {978-3-9819263-4-7},
	url = {https://ieeexplore.ieee.org/document/9116352/},
	doi = {10.23919/DATE48585.2020.9116352},
	shorttitle = {{NeuronFlow}},
	eventtitle = {2020 Design, Automation \& Test in Europe Conference \& Exhibition ({DATE})},
	pages = {840--845},
	booktitle = {2020 Design, Automation \& Test in Europe Conference \& Exhibition ({DATE})},
	publisher = {{IEEE}},
	author = {Moreira, Orlando and Yousefzadeh, Amirreza and Chersi, Fabian and Cinserin, Gokturk and Zwartenkot, Rik-Jan and Kapoor, Ajay and Qiao, Peng and Kievits, Peter and Khoei, Mina and Rouillard, Louis and Ferouge, Aimee and Tapson, Jonathan and Visweswara, Ashoka},
	urldate = {2024-04-05},
	date = {2020-03},
}

@online{noauthor_epoch_nodate,
	title = {Epoch Database Visualization},
	url = {https://epochai.org/data/epochdb/visualization},
	abstract = {Explore Epoch’s database of notable machine learning systems and dive into our interactive visualization to inform your research, work and education. Learn about the training compute, parameters, and training dataset sizes of frontier {AI} systems.},
	titleaddon = {Epoch},
	urldate = {2024-04-05},
	langid = {english},
}

@inproceedings{huang_swapadvisor_2020,
	location = {Lausanne Switzerland},
	title = {{SwapAdvisor}: Pushing Deep Learning Beyond the {GPU} Memory Limit via Smart Swapping},
	isbn = {978-1-4503-7102-5},
	url = {https://dl.acm.org/doi/10.1145/3373376.3378530},
	doi = {10.1145/3373376.3378530},
	shorttitle = {{SwapAdvisor}},
	eventtitle = {{ASPLOS} '20: Architectural Support for Programming Languages and Operating Systems},
	pages = {1341--1355},
	booktitle = {Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems},
	publisher = {{ACM}},
	author = {Huang, Chien-Chin and Jin, Gu and Li, Jinyang},
	urldate = {2024-04-04},
	date = {2020-03-09},
	langid = {english},
}

@article{wang_swapnet_2024,
	title = {{SwapNet}: Efficient Swapping for {DNN} Inference on Edge {AI} Devices Beyond the Memory Budget},
	rights = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/{IEEE}.html},
	issn = {1536-1233, 1558-0660, 2161-9875},
	url = {https://ieeexplore.ieee.org/document/10403957/},
	doi = {10.1109/TMC.2024.3355764},
	shorttitle = {{SwapNet}},
	pages = {1--14},
	journaltitle = {{IEEE} Transactions on Mobile Computing},
	shortjournal = {{IEEE} Trans. on Mobile Comput.},
	author = {Wang, Kun and Cao, Jiani and Zhou, Zimu and Li, Zhenjiang},
	urldate = {2024-04-04},
	date = {2024},
}

@misc{neill_overview_2020,
	title = {An Overview of Neural Network Compression},
	url = {http://arxiv.org/abs/2006.03669},
	abstract = {Overparameterized networks trained to convergence have shown impressive performance in domains such as computer vision and natural language processing. Pushing state of the art on salient tasks within these domains corresponds to these models becoming larger and more difficult for machine learning practitioners to use given the increasing memory and storage requirements, not to mention the larger carbon footprint. Thus, in recent years there has been a resurgence in model compression techniques, particularly for deep convolutional neural networks and self-attention based networks such as the Transformer. Hence, this paper provides a timely overview of both old and current compression techniques for deep neural networks, including pruning, quantization, tensor decomposition, knowledge distillation and combinations thereof. We assume a basic familiarity with deep learning architectures{\textbackslash}footnote\{For an introduction to deep learning, see {\textasciitilde}{\textbackslash}citet\{goodfellow2016deep\}\}, namely, Recurrent Neural Networks{\textasciitilde}{\textbackslash}citep[({RNNs})][]\{rumelhart1985learning,hochreiter1997long\}, Convolutional Neural Networks{\textasciitilde}{\textbackslash}citep\{fukushima1980neocognitron\}{\textasciitilde}{\textbackslash}footnote\{For an up to date overview see{\textasciitilde}{\textbackslash}citet\{khan2019survey\}\} and Self-Attention based networks{\textasciitilde}{\textbackslash}citep\{vaswani2017attention\}{\textbackslash}footnote\{For a general overview of self-attention networks, see {\textasciitilde}{\textbackslash}citet\{chaudhari2019attentive\}.\},{\textbackslash}footnote\{For more detail and their use in natural language processing, see{\textasciitilde}{\textbackslash}citet\{hu2019introductory\}\}. Most of the papers discussed are proposed in the context of at least one of these {DNN} architectures.},
	number = {{arXiv}:2006.03669},
	publisher = {{arXiv}},
	author = {Neill, James O'},
	urldate = {2024-04-04},
	date = {2020-08-01},
	eprinttype = {arxiv},
	eprint = {2006.03669 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{yu_intelligent_2020,
	title = {Intelligent Edge: Leveraging Deep Imitation Learning for Mobile Edge Computation Offloading},
	volume = {27},
	rights = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/{IEEE}.html},
	issn = {1536-1284, 1558-0687},
	url = {https://ieeexplore.ieee.org/document/9023929/},
	doi = {10.1109/MWC.001.1900232},
	shorttitle = {Intelligent Edge},
	pages = {92--99},
	number = {1},
	journaltitle = {{IEEE} Wireless Communications},
	shortjournal = {{IEEE} Wireless Commun.},
	author = {Yu, Shuai and Chen, Xu and Yang, Lei and Wu, Di and Bennis, Mehdi and Zhang, Junshan},
	urldate = {2024-04-04},
	date = {2020-02},
}

@article{xu_deepwear_2020,
	title = {{DeepWear}: Adaptive Local Offloading for On-Wearable Deep Learning},
	volume = {19},
	rights = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/{IEEE}.html},
	issn = {1536-1233, 1558-0660, 2161-9875},
	url = {https://ieeexplore.ieee.org/document/8618364/},
	doi = {10.1109/TMC.2019.2893250},
	shorttitle = {{DeepWear}},
	pages = {314--330},
	number = {2},
	journaltitle = {{IEEE} Transactions on Mobile Computing},
	shortjournal = {{IEEE} Trans. on Mobile Comput.},
	author = {Xu, Mengwei and Qian, Feng and Zhu, Mengze and Huang, Feifan and Pushp, Saumay and Liu, Xuanzhe},
	urldate = {2024-04-04},
	date = {2020-02-01},
}

@inproceedings{drolia_precog_2017,
	location = {San Jose California},
	title = {Precog: prefetching for image recognition applications at the edge},
	isbn = {978-1-4503-5087-7},
	url = {https://dl.acm.org/doi/10.1145/3132211.3134456},
	doi = {10.1145/3132211.3134456},
	shorttitle = {Precog},
	eventtitle = {{SEC} '17: {IEEE}/{ACM} Symposium on Edge Computing},
	pages = {1--13},
	booktitle = {Proceedings of the Second {ACM}/{IEEE} Symposium on Edge Computing},
	publisher = {{ACM}},
	author = {Drolia, Utsav and Guo, Katherine and Narasimhan, Priya},
	urldate = {2024-04-04},
	date = {2017-10-12},
	langid = {english},
}

@inproceedings{ran_deepdecision_2018,
	location = {Honolulu, {HI}},
	title = {{DeepDecision}: A Mobile Deep Learning Framework for Edge Video Analytics},
	isbn = {978-1-5386-4128-6},
	url = {https://ieeexplore.ieee.org/document/8485905/},
	doi = {10.1109/INFOCOM.2018.8485905},
	shorttitle = {{DeepDecision}},
	eventtitle = {{IEEE} {INFOCOM} 2018 - {IEEE} Conference on Computer Communications},
	pages = {1421--1429},
	booktitle = {{IEEE} {INFOCOM} 2018 - {IEEE} Conference on Computer Communications},
	publisher = {{IEEE}},
	author = {Ran, Xukan and Chen, Haolianz and Zhu, Xiaodan and Liu, Zhenming and Chen, Jiasi},
	urldate = {2024-04-04},
	date = {2018-04},
}

@inproceedings{liu_edgeeye_2018,
	location = {Munich Germany},
	title = {{EdgeEye}: An Edge Service Framework for Real-time Intelligent Video Analytics},
	isbn = {978-1-4503-5837-8},
	url = {https://dl.acm.org/doi/10.1145/3213344.3213345},
	doi = {10.1145/3213344.3213345},
	shorttitle = {{EdgeEye}},
	eventtitle = {{MobiSys} '18: The 16th Annual International Conference on Mobile Systems, Applications, and Services},
	pages = {1--6},
	booktitle = {Proceedings of the 1st International Workshop on Edge Systems, Analytics and Networking},
	publisher = {{ACM}},
	author = {Liu, Peng and Qi, Bozhao and Banerjee, Suman},
	urldate = {2024-04-04},
	date = {2018-06-10},
	langid = {english},
}

@inproceedings{liu_edge_2019,
	location = {Los Cabos Mexico},
	title = {Edge Assisted Real-time Object Detection for Mobile Augmented Reality},
	isbn = {978-1-4503-6169-9},
	url = {https://dl.acm.org/doi/10.1145/3300061.3300116},
	doi = {10.1145/3300061.3300116},
	eventtitle = {{MobiCom} '19: The 25th Annual International Conference on Mobile Computing and Networking},
	pages = {1--16},
	booktitle = {The 25th Annual International Conference on Mobile Computing and Networking},
	publisher = {{ACM}},
	author = {Liu, Luyang and Li, Hongyu and Gruteser, Marco},
	urldate = {2024-04-04},
	date = {2019-08-05},
	langid = {english},
}

@thesis{killebrew_l2_2008,
	title = {L2 Cache to Oﬀ-chip Memory Networks for Chip Multiprocessors},
	url = {http://www2.eecs.berkeley.edu/Pubs/TechRpts/2008/EECS-2008-71.html},
	institution = {University of California at Berkeley},
	type = {phdthesis},
	author = {Killebrew, Carrell D},
	date = {2008-05-23},
	langid = {english},
}

@thesis{zhou_performance_2009,
	title = {Performance evaluation of network-on-chip interconnect architectures},
	url = {https://digitalscholarship.unlv.edu/thesesdissertations/63/},
	institution = {University of Nevada Las Vegas},
	type = {phdthesis},
	author = {Zhou, Xinan},
	date = {2009-08},
	langid = {english},
}

@article{paramasivam_network_2015,
	title = {{NETWORK} {ON}-{CHIP} {AND} {ITS} {RESEARCH} {CHALLENGES}},
	volume = {01},
	issn = {23951672, 23951680},
	url = {http://ictactjournals.in/ArticleDetails.aspx?id=1946},
	doi = {10.21917/ijme.2015.0015},
	pages = {83--87},
	number = {2},
	journaltitle = {{ICTACT} Journal on Microelectronics},
	shortjournal = {{IJME}},
	author = {Paramasivam, K.},
	urldate = {2024-03-13},
	date = {2015-07-01},
}

@article{barchi_flexible_2021,
	title = {Flexible On-Line Reconfiguration of Multi-Core Neuromorphic Platforms},
	volume = {9},
	issn = {2168-6750, 2376-4562},
	url = {https://ieeexplore.ieee.org/document/8676216/},
	doi = {10.1109/TETC.2019.2908079},
	pages = {915--927},
	number = {2},
	journaltitle = {{IEEE} Transactions on Emerging Topics in Computing},
	shortjournal = {{IEEE} Trans. Emerg. Topics Comput.},
	author = {Barchi, Francesco and Urgese, Gianvito and Siino, Alessandro and Cataldo, Santa Di and Macii, Enrico and Acquaviva, Andrea},
	urldate = {2024-03-13},
	date = {2021-04-01},
}

@article{ben_survey_2013,
	title = {A Survey of Network-On-Chip Tools},
	volume = {4},
	issn = {2158107X, 21565570},
	url = {http://thesai.org/Publications/ViewPaper?Volume=4&Issue=9&Code=IJACSA&SerialNo=10},
	doi = {10.14569/IJACSA.2013.040910},
	number = {9},
	journaltitle = {International Journal of Advanced Computer Science and Applications},
	shortjournal = {{IJACSA}},
	author = {Ben, Ahmed and Ben, Slim},
	urldate = {2024-03-13},
	date = {2013},
	langid = {english},
}

@article{khan_comparative_2018,
	title = {Comparative analysis of network‐on‐chip simulation tools},
	volume = {12},
	issn = {1751-8601, 1751-861X},
	url = {https://onlinelibrary.wiley.com/doi/10.1049/iet-cdt.2017.0068},
	doi = {10.1049/iet-cdt.2017.0068},
	pages = {30--38},
	number = {1},
	journaltitle = {{IET} Computers \& Digital Techniques},
	shortjournal = {{IET} Computers \&amp; Digital Techniques},
	author = {Khan, Sarzamin and Anjum, Sheraz and Gulzari, Usman Ali and Torres, Frank Sill},
	urldate = {2024-03-13},
	date = {2018-01},
	langid = {english},
}

@inproceedings{abts_achieving_2009,
	location = {Austin {TX} {USA}},
	title = {Achieving predictable performance through better memory controller placement in many-core {CMPs}},
	isbn = {978-1-60558-526-0},
	url = {https://dl.acm.org/doi/10.1145/1555754.1555810},
	doi = {10.1145/1555754.1555810},
	eventtitle = {{ISCA} '09: The 36th Annual International Symposium on Computer Architecture},
	pages = {451--461},
	booktitle = {Proceedings of the 36th annual international symposium on Computer architecture},
	publisher = {{ACM}},
	author = {Abts, Dennis and Enright Jerger, Natalie D. and Kim, John and Gibson, Dan and Lipasti, Mikko H.},
	urldate = {2024-03-13},
	date = {2009-06-20},
	langid = {english},
}

@inproceedings{thanh-vu_le-van_simulation_2012,
	location = {Ha Noi, Vietnam},
	title = {Simulation and performance evaluation of a Network-on-Chip architecture based on {SystemC}},
	isbn = {978-1-4673-4352-7 978-1-4673-4351-0},
	url = {http://ieeexplore.ieee.org/document/6404252/},
	doi = {10.1109/ATC.2012.6404252},
	eventtitle = {2012 International Conference on Advanced Technologies for Communications ({ATC} 2012)},
	pages = {170--175},
	booktitle = {The 2012 International Conference on Advanced Technologies for Communications},
	publisher = {{IEEE}},
	author = {{Thanh-Vu Le-Van} and {Xuan-Tu Tran} and {Dien-Tap Ngo}},
	urldate = {2024-03-13},
	date = {2012-10},
}

@inproceedings{xu_optimal_2011,
	location = {Taipei Taiwan},
	title = {Optimal memory controller placement for chip multiprocessor},
	isbn = {978-1-4503-0715-4},
	url = {https://dl.acm.org/doi/10.1145/2039370.2039405},
	doi = {10.1145/2039370.2039405},
	eventtitle = {{ESWeek} '11: Seventh Embedded Systems Week},
	pages = {217--226},
	booktitle = {Proceedings of the seventh {IEEE}/{ACM}/{IFIP} international conference on Hardware/software codesign and system synthesis},
	publisher = {{ACM}},
	author = {Xu, Thomas Canhao and Liljeberg, Pasi and Tenhunen, Hannu},
	urldate = {2024-03-13},
	date = {2011-10-09},
	langid = {english},
}

@article{furber_spinnaker_2014,
	title = {The {SpiNNaker} Project},
	volume = {102},
	issn = {0018-9219, 1558-2256},
	url = {https://ieeexplore.ieee.org/document/6750072/},
	doi = {10.1109/JPROC.2014.2304638},
	pages = {652--665},
	number = {5},
	journaltitle = {Proceedings of the {IEEE}},
	shortjournal = {Proc. {IEEE}},
	author = {Furber, Steve B. and Galluppi, Francesco and Temple, Steve and Plana, Luis A.},
	urldate = {2024-03-13},
	date = {2014-05},
}

@collection{furber_spinnaker_2020,
	title = {{SpiNNaker}: A Spiking Neural Network Architecture},
	isbn = {978-1-68083-652-3 978-1-68083-653-0},
	url = {https://nowpublishers.com/article/BookDetails/9781680836523},
	shorttitle = {{SpiNNaker}},
	publisher = {Now Publishers},
	editor = {Furber, Steve and Bogdan, Petrut},
	urldate = {2024-03-13},
	date = {2020},
	doi = {10.1561/9781680836523},
}

@article{seema_network--chip_2017,
	title = {Network-on-Chip: A State-of-the-art Review},
	volume = {07},
	issn = {23194197, 23194200},
	url = {http://www.iosrjournals.org/iosr-jvlsi/papers/vol7-issue4/Version-1/E0704012935.pdf},
	doi = {10.9790/4200-0704012935},
	shorttitle = {Network-on-Chip},
	pages = {29--35},
	number = {4},
	journaltitle = {{IOSR} Journal of {VLSI} and Signal Processing},
	shortjournal = {{IOSR} {JVSP}},
	author = {Seema, Seema and Dahiya, Pawan Kumar},
	urldate = {2024-03-13},
	date = {2017-07},
}

@article{wentzlaff_-chip_2007,
	title = {On-Chip Interconnection Architecture of the Tile Processor},
	volume = {27},
	issn = {0272-1732},
	url = {http://ieeexplore.ieee.org/document/4378780/},
	doi = {10.1109/MM.2007.4378780},
	pages = {15--31},
	number = {5},
	journaltitle = {{IEEE} Micro},
	shortjournal = {{IEEE} Micro},
	author = {Wentzlaff, D. and Griffin, P. and Hoffmann, H. and {Liewei Bao} and Edwards, B. and Ramey, C. and Mattina, M. and {Chyi-Chang Miao} and Brown, J.F. and Agarwal, A.},
	urldate = {2024-03-13},
	date = {2007-09},
}

@incollection{ma_review_2011,
	location = {Berlin, Heidelberg},
	title = {A Review of Research on Network-on-Chip Simulator},
	volume = {100},
	isbn = {978-3-642-21761-6 978-3-642-21762-3},
	url = {http://link.springer.com/10.1007/978-3-642-21762-3_13},
	pages = {103--110},
	booktitle = {Communication Systems and Information Technology},
	publisher = {Springer Berlin Heidelberg},
	author = {Gu, Haiyun},
	editor = {Ma, Ming},
	urldate = {2024-03-13},
	date = {2011},
	doi = {10.1007/978-3-642-21762-3_13},
	note = {Series Title: Lecture Notes in Electrical Engineering},
}
